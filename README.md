# Speculative Decoding

This repository is a pytorch implementation of Speculative Decoding [(Leviathan et al., 2023)](#1).
It contains the code for two generation strategies: classic auto-regressive decoding and speculative decoding. Both of these generation strategies can be used in a greedy or nucleus sampling (temperature, top k and top p) setting.

Additionally, I provided the code for a custom N-gram Language Model (NLM): usage of N-gram counts in an auto-regressive way. Each step will provide the probability that each token follow the last (n-1)-gram of the input.
This NLM can be used as the drafter in the speculative decoding strategy.

## What is Speculative Decoding?

Speculative Decoding is a decoding strategy for transformers that allows to generate sequences faster than the classic auto-regressive decoding without changing the output distribution or requiring further fine-tuning. It uses a smaller, more efficient approximation model (called a "drafter") to generate speculative token prefixes. These prefixes are then evaluated in parallel by the larger target model, reducing the number of serial decoding steps required and leading to inference speedups.

The core process rely on the specific behavior of the Transformer model that allows to compute the probability distribution of all the fed in tokens. This distribution is then used to verify the drafts generated by the drafter model.

## How to use

### 0. Installation
This project requires Python 3.7 or later and the following dependencies:

```
datasets==2.18.0
numpy==1.26.4
rich==13.7.1
termcolor==2.4.0
tokenizers==0.15.2
torch==2.2.2
tqdm==4.66.2
transformers==4.39.2
```

Simply fork this repository and install the dependencies.

### 1. Generate text using Speculative Decoding

#### a. Load the target and drafter model

The target model is the transformer model we want to accelerate, while the drafter model is the smaller model that will be used to generate drafts to the target model.

Here are some requirements to make speculative decoding work:
- The target model must be a transformer model (encoder-decoder or decoder only).
- The drafter model must share the same tokenizer as the target model.
- The drafter model should be small enough to be faster than the target model.

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# We will use the Flan T5 XXL as the model we want to accelerate (11.3B parameters)
target_model_name = "google/flan-t5-xxl"
target = AutoModelForSeq2SeqLM.from_pretrained(target_model_name)

# We will use the Flan T5 Small as the drafter model (77M parameters)
drafter_model_name = "google/flan-t5-small"
drafter = AutoModelForSeq2SeqLM.from_pretrained(drafter_model_name)

# Don't forget to load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(target_model_name)
```

#### b. Prepare the input

Before generating text, we need to prepare the input. The input should be tokenized and encoded using the tokenizer.

```python
prefix = "Translate French to English: Je m'appelle Romain. N'hésitez pas à contribuer à mon projet !"
input_ids = tokenizer(prefix, return_tensors="pt").input_ids
```

#### c. Generate text

Speculative Decoding uses one hyperparameter: $\gamma$, the number of drafts generated by the drafter model at each step. 

Increasing the value of $\gamma$ will not always lead to a faster generation, as the drafts may be rejected more. The acceptance rate $\alpha$ is the number of drafts accepted by the target model divided by the number of drafts generated. The higher the acceptance rate, the faster the generation. So the idea is to find the ideal $\gamma$ according to the acceptance rate in order to get the fastest generation.

```python
from sampling import speculative_decoding, autoregressive_decoding

# Parameters
gen_len = 100       # Maximum number of tokens generated (could over pass when using speculative decoding)
gamma = 4           # Number of drafts generated by the drafter model at each step
temperature = 1.0   # Temperature for nucleus sampling
top_k = 1           # Top k for nucleus sampling (top k = 1 is equivalent to greedy decoding)
top_p = 0.0         # Top p for nucleus sampling (top p = 0 will disable it)

# Generate text using the classic auto-regressive decoding (slow)
output_ids_ar = autoregressive_decoding(
                input_ids,
                target,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                max_len=gen_len,
                end_token_id=tokenizer.eos_token_id,
            )
output_ar = tokenizer.decode(output_ids_ar[0], skip_special_tokens=True)

# Generate text using the speculative decoding (faster)
output_ids_sd, alpha = speculative_decoding(
                input_ids,
                drafter,
                target,
                gamma=gamma,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                min_len=gen_len,
                end_token_id=tokenizer.eos_token_id,
            )
output_sd = tokenizer.decode(output_ids_sd[0], skip_special_tokens=True)

print("Auto-regressive decoding:", output_ar)
print("Speculative decoding:", output_sd)
print("Acceptance rate:", alpha) # Number of drafts accepted by the target model divided by the number of drafts generated
```

### 2. Run the comparative between the two decoding strategies

Two files are at disposition to compare the two strategies:
- `infer.py`: allows you to generate using your own input
- `evaluate.py`: allows you to evaluate the speed of the two strategies using a dataset

Those files will generate the output, and give the time taken + tokens throughput for each strategy.

### 3. Use the N-gram Language Model (NLM)

The NLM is a pytorch model that counts the n-grams in a corpus (done during the fitting process) and can be used to generate the logits given the last n-1 tokens. This model can be used as the drafter in the speculative decoding.
This NLM also uses backoff NLM ((N-1)LM, ..., 1LM) to generate the logits when the input_ids are too short.
Using a Laplace smoothing, the NLM can generate the logits for any token.

To fit the NLM, you can take a look at the `fit.py` file. It will load a dataset, fit the NLM on it and finally save the model.

Future paths for NLM:
- Following the Infini-gram [(Liu et al., 2024)](#5), select the best N at each step.
- Implement a non-logit NLM to generate the tokens directly.
- Make it possible to fit continuously

## Did you find any bug?

Please open an issue or submit a pull request if you find any bug. Contributions are welcome!

## References
<a id="1">[1]</a> Leviathan, Y., Kalman, M. &amp; Matias, Y.. (2023). Fast Inference from Transformers via Speculative Decoding. <i>Proceedings of the 40th International Conference on Machine Learning</i>, in <i>Proceedings of Machine Learning Research</i> 202:19274-19286 Available from https://proceedings.mlr.press/v202/leviathan23a.html.

<a id="2">[2]</a> Liu, J., Min, S., Zettlemoyer, L., Choi, Y., & Hajishirzi, H. (2024). Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens. <i>arXiv preprint</i> arXiv:2401.17377.