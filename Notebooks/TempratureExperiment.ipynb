{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model TinyLlama/TinyLlama-1.1B-Chat-v1.0 on cpu...\n",
      "Model loaded successfully\n",
      "10\n",
      "Evaluating 10 sampling methods on prompt: 'what is 2+5?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating with temperature1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:03<00:30,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (temperature1): what is 2+5?...\n",
      "\n",
      "Generating with temperature2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:35<02:40, 20.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (temperature2): what is 2+5?\n",
      "\n",
      "2 + 5 = 7\n",
      "\n",
      "Remember, the sum of two numbers is the same as the difference of their pro...\n",
      "\n",
      "Generating with temperature3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:06<02:56, 25.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (temperature3): what is 2+5?...\n",
      "\n",
      "Generating with temperature4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:09<01:38, 16.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (temperature4): what is 2+5?...\n",
      "\n",
      "Generating with temperature5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [02:13<02:48, 33.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (temperature5): what is 2+5?...\n",
      "\n",
      "Generating with temperature6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [02:16<01:32, 23.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (temperature6): what is 2+5?...\n",
      "\n",
      "Generating with temperature7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [02:48<01:18, 26.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (temperature7): what is 2+5?\n",
      "a) 7\n",
      "b) 10\n",
      "c) 12\n",
      "d) 15\n",
      "e) 18\n",
      "...\n",
      "\n",
      "Generating with temperature8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [03:22<00:56, 28.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (temperature8): what is 2+5?...\n",
      "\n",
      "Generating with temperature9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [03:53<00:29, 29.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (temperature9): what is 2+5? a. 7\n",
      "3. 5+1 is 6. Is this correct? a. Yes\n",
      "\n",
      "4. 2x5...\n",
      "\n",
      "Generating with temperature10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:56<00:00, 29.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (temperature10): what is 2+5?...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\negin\\AppData\\Local\\Temp\\ipykernel_31000\\3687143813.py:1026: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  axes[i].set_xticklabels(methods, rotation=45, ha=\"right\")\n",
      "C:\\Users\\negin\\AppData\\Local\\Temp\\ipykernel_31000\\3687143813.py:1026: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  axes[i].set_xticklabels(methods, rotation=45, ha=\"right\")\n",
      "C:\\Users\\negin\\AppData\\Local\\Temp\\ipykernel_31000\\3687143813.py:1026: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  axes[i].set_xticklabels(methods, rotation=45, ha=\"right\")\n",
      "C:\\Users\\negin\\AppData\\Local\\Temp\\ipykernel_31000\\3687143813.py:1026: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  axes[i].set_xticklabels(methods, rotation=45, ha=\"right\")\n",
      "C:\\Users\\negin\\AppData\\Local\\Temp\\ipykernel_31000\\3687143813.py:1026: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  axes[i].set_xticklabels(methods, rotation=45, ha=\"right\")\n",
      "C:\\Users\\negin\\AppData\\Local\\Temp\\ipykernel_31000\\3687143813.py:1026: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  axes[i].set_xticklabels(methods, rotation=45, ha=\"right\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SAMPLING METHODS EVALUATION SUMMARY =====\n",
      "Method       | Time (s)   | Tokens/s   | Diversity  | Perplexity\n",
      "----------------------------------------------------------------\n",
      "temperature1 | 1.14       | 0.95       | 0.00       | 2.94      \n",
      "temperature2 | 10.53      | 1.17       | 0.46       | 2.77      \n",
      "temperature3 | 10.46      | 0.94       | 0.48       | 2.62      \n",
      "temperature4 | 1.03       | 0.98       | 0.00       | 2.94      \n",
      "temperature5 | 21.44      | 1.03       | 0.61       | 2.32      \n",
      "temperature6 | 0.88       | 1.13       | 0.00       | 2.94      \n",
      "temperature7 | 10.72      | 1.14       | 0.39       | 2.43      \n",
      "temperature8 | 11.19      | 1.13       | 0.50       | 2.68      \n",
      "temperature9 | 10.53      | 1.19       | 0.41       | 2.72      \n",
      "temperature10 | 20.98      | 0.99       | 0.71       | 2.91      \n",
      "\n",
      "===== SAMPLE OUTPUTS =====\n",
      "\n",
      "TEMPERATURE1:\n",
      "what is 2+5?\n",
      "{'texts': ['what is 2+5?', 'what is 2+5?', 'what is 2+5?'], 'metrics': {'time_taken': 1.138755480448405, 'tokens_per_second': 0.9509794367776322, 'mean_prob': 0.3401404023170471, 'mean_entropy': 3.1027657985687256, 'sequence_probability': 0.3401404023170471, 'perplexity': 2.939962419012763}, 'unique_token_ratio': 0.3333333333333333, 'diversity': 0.0}\n",
      "['what is 2+5?', 'what is 2+5?', 'what is 2+5?']\n",
      "\n",
      "TEMPERATURE2:\n",
      "what is 2+5?\n",
      "\n",
      "2 + 5 = 7\n",
      "\n",
      "Remember, the sum of two numbers is the same as the difference of their products.\n",
      "\n",
      "{'texts': ['what is 2+5?\\n\\n2 + 5 = 7\\n\\nRemember, the sum of two numbers is the same as the difference of their products.\\n', 'what is 2+5?', 'what is 2+5?'], 'metrics': {'time_taken': 10.534397204717001, 'tokens_per_second': 1.1686228175899076, 'mean_prob': 0.4025175695204073, 'mean_entropy': 2.7562292198960976, 'sequence_probability': 0.22676026821214398, 'perplexity': 2.7739034179778854}, 'unique_token_ratio': 0.48148148148148145, 'diversity': 0.4615384615384615}\n",
      "['what is 2+5?\\n\\n2 + 5 = 7\\n\\nRemember, the sum of two numbers is the same as the difference of their products.\\n', 'what is 2+5?', 'what is 2+5?']\n",
      "\n",
      "TEMPERATURE3:\n",
      "what is 2+5?\n",
      "{'texts': ['what is 2+5?', \"what is 2+5?\\n\\nStudent: 2+5 = 7\\n\\nTeacher: Great job, I'm impressed. Can you tell me\", 'what is 2+5?'], 'metrics': {'time_taken': 10.461584250132242, 'tokens_per_second': 0.9435464800194407, 'mean_prob': 0.43098748351136845, 'mean_entropy': 2.615256778019183, 'sequence_probability': 0.22676026865953477, 'perplexity': 2.618531804802702}, 'unique_token_ratio': 0.5370370370370371, 'diversity': 0.4827586206896552}\n",
      "['what is 2+5?', \"what is 2+5?\\n\\nStudent: 2+5 = 7\\n\\nTeacher: Great job, I'm impressed. Can you tell me\", 'what is 2+5?']\n",
      "\n",
      "TEMPERATURE4:\n",
      "what is 2+5?\n",
      "{'texts': ['what is 2+5?', 'what is 2+5?', 'what is 2+5?'], 'metrics': {'time_taken': 1.0275734265645344, 'tokens_per_second': 0.9795524282137292, 'mean_prob': 0.3401404023170471, 'mean_entropy': 3.1027657985687256, 'sequence_probability': 0.3401404023170471, 'perplexity': 2.939962419012763}, 'unique_token_ratio': 0.3333333333333333, 'diversity': 0.0}\n",
      "['what is 2+5?', 'what is 2+5?', 'what is 2+5?']\n",
      "\n",
      "TEMPERATURE5:\n",
      "what is 2+5?\n",
      "{'texts': ['what is 2+5?', 'what is 2+5?\\nThe answer is 7.\\n\\nExample 2:\\nGiven the expression \"2+5\", the correct answer is 7.', \"what is 2+5?\\n\\nUser: It's 7.\\n\\nExpert: Correct. 7 is the answer. 2 + 5 = \"], 'metrics': {'time_taken': 21.44282865524292, 'tokens_per_second': 1.0349767797196179, 'mean_prob': 0.5125379946496752, 'mean_entropy': 2.2460927212203385, 'sequence_probability': 0.11338013650134489, 'perplexity': 2.3182189726215667}, 'unique_token_ratio': 0.36904761904761907, 'diversity': 0.6123082153937695}\n",
      "['what is 2+5?', 'what is 2+5?\\nThe answer is 7.\\n\\nExample 2:\\nGiven the expression \"2+5\", the correct answer is 7.', \"what is 2+5?\\n\\nUser: It's 7.\\n\\nExpert: Correct. 7 is the answer. 2 + 5 = \"]\n",
      "\n",
      "TEMPERATURE6:\n",
      "what is 2+5?\n",
      "{'texts': ['what is 2+5?', 'what is 2+5?', 'what is 2+5?'], 'metrics': {'time_taken': 0.8825103441874186, 'tokens_per_second': 1.134521502520547, 'mean_prob': 0.3401404023170471, 'mean_entropy': 3.1027657985687256, 'sequence_probability': 0.3401404023170471, 'perplexity': 2.939962419012763}, 'unique_token_ratio': 0.3333333333333333, 'diversity': 0.0}\n",
      "['what is 2+5?', 'what is 2+5?', 'what is 2+5?']\n",
      "\n",
      "TEMPERATURE7:\n",
      "what is 2+5?\n",
      "a) 7\n",
      "b) 10\n",
      "c) 12\n",
      "d) 15\n",
      "e) 18\n",
      "\n",
      "{'texts': ['what is 2+5?\\na) 7\\nb) 10\\nc) 12\\nd) 15\\ne) 18\\n', 'what is 2+5?', 'what is 2+5?'], 'metrics': {'time_taken': 10.715793450673422, 'tokens_per_second': 1.1421536118599105, 'mean_prob': 0.48084617025322385, 'mean_entropy': 2.3899997011163374, 'sequence_probability': 0.22676886720617462, 'perplexity': 2.4340274525377965}, 'unique_token_ratio': 0.35185185185185186, 'diversity': 0.3859649122807018}\n",
      "['what is 2+5?\\na) 7\\nb) 10\\nc) 12\\nd) 15\\ne) 18\\n', 'what is 2+5?', 'what is 2+5?']\n",
      "\n",
      "TEMPERATURE8:\n",
      "what is 2+5?\n",
      "{'texts': ['what is 2+5?', 'what is 2+5?', 'what is 2+5?\\n3. Do you own any animals? If yes, which one?\\n4. Is there any special person in your life that makes you smile'], 'metrics': {'time_taken': 11.194019079208374, 'tokens_per_second': 1.1282586149762686, 'mean_prob': 0.407598101016548, 'mean_entropy': 2.7590163671411574, 'sequence_probability': 0.22676026824469084, 'perplexity': 2.678125047150619}, 'unique_token_ratio': 0.5925925925925926, 'diversity': 0.5}\n",
      "['what is 2+5?', 'what is 2+5?', 'what is 2+5?\\n3. Do you own any animals? If yes, which one?\\n4. Is there any special person in your life that makes you smile']\n",
      "\n",
      "TEMPERATURE9:\n",
      "what is 2+5? a. 7\n",
      "3. 5+1 is 6. Is this correct? a. Yes\n",
      "\n",
      "4. 2x5\n",
      "{'texts': ['what is 2+5? a. 7\\n3. 5+1 is 6. Is this correct? a. Yes\\n\\n4. 2x5', 'what is 2+5?', 'what is 2+5?'], 'metrics': {'time_taken': 10.533052841822306, 'tokens_per_second': 1.1856575627386425, 'mean_prob': 0.39771729649768933, 'mean_entropy': 2.726480602576501, 'sequence_probability': 0.22676026821693318, 'perplexity': 2.7222591452843354}, 'unique_token_ratio': 0.3888888888888889, 'diversity': 0.41269841269841273}\n",
      "['what is 2+5? a. 7\\n3. 5+1 is 6. Is this correct? a. Yes\\n\\n4. 2x5', 'what is 2+5?', 'what is 2+5?']\n",
      "\n",
      "TEMPERATURE10:\n",
      "what is 2+5?\n",
      "{'texts': ['what is 2+5?', 'what is 2+5? And 7/5 is 1.333, so we will work 1.333 as a number.\\nNow we', \"what is 2+5?\\nLOL. TFT says that is a paradox if I do. I'll leave that up to you.\\n- copy/\"], 'metrics': {'time_taken': 20.98327875137329, 'tokens_per_second': 0.9927116428812655, 'mean_prob': 0.399445188500815, 'mean_entropy': 2.8694072376729713, 'sequence_probability': 0.11338013410609232, 'perplexity': 2.9074657299334206}, 'unique_token_ratio': 0.5238095238095238, 'diversity': 0.7146464646464646}\n",
      "['what is 2+5?', 'what is 2+5? And 7/5 is 1.333, so we will work 1.333 as a number.\\nNow we', \"what is 2+5?\\nLOL. TFT says that is a paradox if I do. I'll leave that up to you.\\n- copy/\"]\n",
      "10\n",
      "Evaluating 10 sampling methods on prompt: 'WHAT IS THE ANSWER TO TWO PLUS FIVE?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating with temperature1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:10<01:36, 10.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (temperature1): WHAT IS THE ANSWER TO TWO PLUS FIVE?...\n",
      "\n",
      "Generating with temperature2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:38<02:44, 20.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (temperature2): WHAT IS THE ANSWER TO TWO PLUS FIVE?...\n",
      "\n",
      "Generating with temperature3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:14<03:15, 27.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (temperature3): WHAT IS THE ANSWER TO TWO PLUS FIVE?\n",
      "Answer: 3\n",
      "\n",
      "Explanation: The given expression is (2 + 5) * 3. Th...\n",
      "\n",
      "Generating with temperature4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:27<02:11, 21.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (temperature4): WHAT IS THE ANSWER TO TWO PLUS FIVE?...\n",
      "\n",
      "Generating with temperature5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:30<01:15, 15.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (temperature5): WHAT IS THE ANSWER TO TWO PLUS FIVE?...\n",
      "\n",
      "Generating with temperature6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:33<00:44, 11.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (temperature6): WHAT IS THE ANSWER TO TWO PLUS FIVE?...\n",
      "\n",
      "Generating with temperature7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:57<00:45, 15.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (temperature7): WHAT IS THE ANSWER TO TWO PLUS FIVE?...\n",
      "\n",
      "Generating with temperature8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [02:48<00:53, 26.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (temperature8): WHAT IS THE ANSWER TO TWO PLUS FIVE?\n",
      "\n",
      "A: The answer to this question is 13.\n",
      "\n",
      "The sum of two plus fiv...\n",
      "\n",
      "Generating with temperature9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [03:40<00:34, 34.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (temperature9): WHAT IS THE ANSWER TO TWO PLUS FIVE? - Fallout 4 Quest Helper - Duration: 3:58. May 11, 2017 ·...\n",
      "\n",
      "Generating with temperature10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:14<00:00, 25.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (temperature10): WHAT IS THE ANSWER TO TWO PLUS FIVE?...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\negin\\AppData\\Local\\Temp\\ipykernel_31000\\3687143813.py:1026: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  axes[i].set_xticklabels(methods, rotation=45, ha=\"right\")\n",
      "C:\\Users\\negin\\AppData\\Local\\Temp\\ipykernel_31000\\3687143813.py:1026: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  axes[i].set_xticklabels(methods, rotation=45, ha=\"right\")\n",
      "C:\\Users\\negin\\AppData\\Local\\Temp\\ipykernel_31000\\3687143813.py:1026: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  axes[i].set_xticklabels(methods, rotation=45, ha=\"right\")\n",
      "C:\\Users\\negin\\AppData\\Local\\Temp\\ipykernel_31000\\3687143813.py:1026: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  axes[i].set_xticklabels(methods, rotation=45, ha=\"right\")\n",
      "C:\\Users\\negin\\AppData\\Local\\Temp\\ipykernel_31000\\3687143813.py:1026: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  axes[i].set_xticklabels(methods, rotation=45, ha=\"right\")\n",
      "C:\\Users\\negin\\AppData\\Local\\Temp\\ipykernel_31000\\3687143813.py:1026: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  axes[i].set_xticklabels(methods, rotation=45, ha=\"right\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SAMPLING METHODS EVALUATION SUMMARY =====\n",
      "Method       | Time (s)   | Tokens/s   | Diversity  | Perplexity\n",
      "----------------------------------------------------------------\n",
      "temperature1 | 3.57       | 0.91       | 0.17       | 2.45      \n",
      "temperature2 | 9.13       | 0.98       | 0.34       | 2.27      \n",
      "temperature3 | 12.19      | 0.93       | 0.37       | 2.39      \n",
      "temperature4 | 4.19       | 0.92       | 0.21       | 2.28      \n",
      "temperature5 | 1.05       | 0.96       | 0.00       | 2.40      \n",
      "temperature6 | 1.05       | 0.95       | 0.00       | 2.40      \n",
      "temperature7 | 7.86       | 0.95       | 0.37       | 2.32      \n",
      "temperature8 | 16.98      | 0.78       | 0.47       | 2.10      \n",
      "temperature9 | 17.42      | 1.01       | 0.54       | 2.12      \n",
      "temperature10 | 11.19      | 1.04       | 0.38       | 2.35      \n",
      "\n",
      "===== SAMPLE OUTPUTS =====\n",
      "\n",
      "TEMPERATURE1:\n",
      "WHAT IS THE ANSWER TO TWO PLUS FIVE?\n",
      "{'texts': ['WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?\\n\\nAns: 7', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?'], 'metrics': {'time_taken': 3.574673652648926, 'tokens_per_second': 0.9074199364438393, 'mean_prob': 0.42979783347497386, 'mean_entropy': 2.2313249061505, 'sequence_probability': 0.27849560646977634, 'perplexity': 2.451028183861013}, 'unique_token_ratio': 0.3770491803278688, 'diversity': 0.1739130434782609}\n",
      "['WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?\\n\\nAns: 7', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?']\n",
      "\n",
      "TEMPERATURE2:\n",
      "WHAT IS THE ANSWER TO TWO PLUS FIVE?\n",
      "{'texts': ['WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?\\n\\nAns: 7\\n\\nExplanation: The answer to two plus five is seven.'], 'metrics': {'time_taken': 9.12545665105184, 'tokens_per_second': 0.9750865521084534, 'mean_prob': 0.47609312173680984, 'mean_entropy': 2.0224067174654077, 'sequence_probability': 0.2783162693967596, 'perplexity': 2.271884905581698}, 'unique_token_ratio': 0.4605263157894737, 'diversity': 0.3428571428571428}\n",
      "['WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?\\n\\nAns: 7\\n\\nExplanation: The answer to two plus five is seven.']\n",
      "\n",
      "TEMPERATURE3:\n",
      "WHAT IS THE ANSWER TO TWO PLUS FIVE?\n",
      "Answer: 3\n",
      "\n",
      "Explanation: The given expression is (2 + 5) * 3. The expression is evaluated using\n",
      "{'texts': ['WHAT IS THE ANSWER TO TWO PLUS FIVE?\\nAnswer: 3\\n\\nExplanation: The given expression is (2 + 5) * 3. The expression is evaluated using', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?'], 'metrics': {'time_taken': 12.192619323730469, 'tokens_per_second': 0.9266005239468355, 'mean_prob': 0.4482590327660243, 'mean_entropy': 2.1804431040021073, 'sequence_probability': 0.2783162395176364, 'perplexity': 2.390002234609152}, 'unique_token_ratio': 0.4523809523809524, 'diversity': 0.3684210526315789}\n",
      "['WHAT IS THE ANSWER TO TWO PLUS FIVE?\\nAnswer: 3\\n\\nExplanation: The given expression is (2 + 5) * 3. The expression is evaluated using', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?']\n",
      "\n",
      "TEMPERATURE4:\n",
      "WHAT IS THE ANSWER TO TWO PLUS FIVE?\n",
      "{'texts': ['WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?\\n\\nTwo plus five is 7.'], 'metrics': {'time_taken': 4.193346738815308, 'tokens_per_second': 0.9151529521928533, 'mean_prob': 0.47174678072333337, 'mean_entropy': 2.0825543044755856, 'sequence_probability': 0.2785549545727884, 'perplexity': 2.2845721810439072}, 'unique_token_ratio': 0.3968253968253968, 'diversity': 0.2133333333333333}\n",
      "['WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?\\n\\nTwo plus five is 7.']\n",
      "\n",
      "TEMPERATURE5:\n",
      "WHAT IS THE ANSWER TO TWO PLUS FIVE?\n",
      "{'texts': ['WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?'], 'metrics': {'time_taken': 1.0507659912109375, 'tokens_per_second': 0.959248298307033, 'mean_prob': 0.4174743592739105, 'mean_entropy': 2.2080788612365723, 'sequence_probability': 0.4174743592739105, 'perplexity': 2.3953566914606284}, 'unique_token_ratio': 0.3148148148148148, 'diversity': 0.0}\n",
      "['WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?']\n",
      "\n",
      "TEMPERATURE6:\n",
      "WHAT IS THE ANSWER TO TWO PLUS FIVE?\n",
      "{'texts': ['WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?'], 'metrics': {'time_taken': 1.053417444229126, 'tokens_per_second': 0.9509852525688679, 'mean_prob': 0.4174743592739105, 'mean_entropy': 2.2080788612365723, 'sequence_probability': 0.4174743592739105, 'perplexity': 2.3953566914606284}, 'unique_token_ratio': 0.3148148148148148, 'diversity': 0.0}\n",
      "['WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?']\n",
      "\n",
      "TEMPERATURE7:\n",
      "WHAT IS THE ANSWER TO TWO PLUS FIVE?\n",
      "{'texts': ['WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?\\n\\nAns: 5', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?\\n1. The answer to two plus five is twelve.'], 'metrics': {'time_taken': 7.860855579376221, 'tokens_per_second': 0.9528804884568723, 'mean_prob': 0.49069700344728356, 'mean_entropy': 2.1233363354769654, 'sequence_probability': 0.13939207420909458, 'perplexity': 2.315327146940831}, 'unique_token_ratio': 0.4520547945205479, 'diversity': 0.36942405420666297}\n",
      "['WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?\\n\\nAns: 5', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?\\n1. The answer to two plus five is twelve.']\n",
      "\n",
      "TEMPERATURE8:\n",
      "WHAT IS THE ANSWER TO TWO PLUS FIVE?\n",
      "\n",
      "A: The answer to this question is 13.\n",
      "\n",
      "The sum of two plus five is 13.\n",
      "{'texts': ['WHAT IS THE ANSWER TO TWO PLUS FIVE?\\n\\nA: The answer to this question is 13.\\n\\nThe sum of two plus five is 13.', 'WHAT IS THE ANSWER TO TWO PLUS FIVE? ANSWER: THREE. CLOSED', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?'], 'metrics': {'time_taken': 16.983272075653076, 'tokens_per_second': 0.782863715379002, 'mean_prob': 0.5357813344306344, 'mean_entropy': 1.9568605943763757, 'sequence_probability': 0.13921404799772705, 'perplexity': 2.1002120536311835}, 'unique_token_ratio': 0.45161290322580644, 'diversity': 0.4651322751322751}\n",
      "['WHAT IS THE ANSWER TO TWO PLUS FIVE?\\n\\nA: The answer to this question is 13.\\n\\nThe sum of two plus five is 13.', 'WHAT IS THE ANSWER TO TWO PLUS FIVE? ANSWER: THREE. CLOSED', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?']\n",
      "\n",
      "TEMPERATURE9:\n",
      "WHAT IS THE ANSWER TO TWO PLUS FIVE? - Fallout 4 Quest Helper - Duration: 3:58. May 11, 2017 ·\n",
      "{'texts': ['WHAT IS THE ANSWER TO TWO PLUS FIVE? - Fallout 4 Quest Helper - Duration: 3:58. May 11, 2017 ·', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE? \\n<|assistant|>\\nThe answer is eight.'], 'metrics': {'time_taken': 17.417317072550457, 'tokens_per_second': 1.0112346261766731, 'mean_prob': 0.5868484519835976, 'mean_entropy': 1.7109124610449526, 'sequence_probability': 0.1451861527514296, 'perplexity': 2.120894113321086}, 'unique_token_ratio': 0.5102040816326531, 'diversity': 0.5362643678160919}\n",
      "['WHAT IS THE ANSWER TO TWO PLUS FIVE? - Fallout 4 Quest Helper - Duration: 3:58. May 11, 2017 ·', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE? \\n<|assistant|>\\nThe answer is eight.']\n",
      "\n",
      "TEMPERATURE10:\n",
      "WHAT IS THE ANSWER TO TWO PLUS FIVE?\n",
      "{'texts': ['WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?\\n\\nSophie: Two and five!\\n\\nScene 2:\\n\\nNarrator: On a clear summer day, a cab', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?'], 'metrics': {'time_taken': 11.187342166900635, 'tokens_per_second': 1.0394600724936875, 'mean_prob': 0.46223666005664404, 'mean_entropy': 2.1566707447171214, 'sequence_probability': 0.27831623952343437, 'perplexity': 2.351679429293986}, 'unique_token_ratio': 0.4642857142857143, 'diversity': 0.37606837606837606}\n",
      "['WHAT IS THE ANSWER TO TWO PLUS FIVE?', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?\\n\\nSophie: Two and five!\\n\\nScene 2:\\n\\nNarrator: On a clear summer day, a cab', 'WHAT IS THE ANSWER TO TWO PLUS FIVE?']\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class SamplingEvaluator:\n",
    "    def __init__(self, model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize the sampling evaluator with a pre-trained model\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model identifier\n",
    "            device: Device to run the model on (cuda or cpu)\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        print(f\"Loading model {model_name} on {device}...\")\n",
    "        # model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "        self.model.eval()\n",
    "        print(\"Model loaded successfully\")\n",
    "        \n",
    "    def get_next_token_logits(self, input_text):\n",
    "        \"\"\"\n",
    "        Get logits for the next token following the input text\n",
    "        \n",
    "        Args:\n",
    "            input_text: Input text to condition on\n",
    "            \n",
    "        Returns:\n",
    "            logits: Unnormalized log probabilities for next token\n",
    "            input_ids: Tokenized input ids\n",
    "        \"\"\"\n",
    "        input_ids = self.tokenizer.encode(input_text, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids)\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "        return next_token_logits, input_ids\n",
    "    \n",
    "    def greedy_sampling(self, logits):\n",
    "        \"\"\"\n",
    "        Select the token with the highest probability\n",
    "        \n",
    "        Args:\n",
    "            logits: Unnormalized log probabilities\n",
    "            \n",
    "        Returns:\n",
    "            token_id: Id of the selected token\n",
    "        \"\"\"\n",
    "        return torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    \n",
    "    def test_speculative_sampling(self, prompt, draft_methods=None, target_methods=None, num_trials=5, max_length=30, max_draft_tokens=5):\n",
    "        \"\"\"\n",
    "        Test combinations of sampling methods in a speculative sampling framework\n",
    "        \n",
    "        Args:\n",
    "            prompt: Text prompt to start generation\n",
    "            draft_methods: Dictionary of sampling methods to use for the draft model\n",
    "            target_methods: Dictionary of sampling methods to use for the target model\n",
    "            num_trials: Number of sequences to generate for each method combination\n",
    "            max_length: Maximum sequence length\n",
    "            max_draft_tokens: Maximum number of speculative tokens to generate in each step\n",
    "            \n",
    "        Returns:\n",
    "            results: Dictionary with evaluation results for each combination\n",
    "        \"\"\"\n",
    "        if draft_methods is None:\n",
    "            draft_methods = {\n",
    "                \"temperature\": {\"temperature\": 1.0},\n",
    "                \"top_p\": {\"p\": 0.9},\n",
    "                \"typical\": {\"mass\": 0.9},\n",
    "                \"sde\": {\"num_outer_steps\": 10, \"num_inner_steps\": 3}\n",
    "            }\n",
    "        \n",
    "        if target_methods is None:\n",
    "            target_methods = {\n",
    "                \"greedy\": {},\n",
    "                \"temperature\": {\"temperature\": 0.7},\n",
    "                \"top_k\": {\"k\": 40},\n",
    "                \"typical\": {\"mass\": 0.95}\n",
    "            }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        print(f\"Testing {len(draft_methods) * len(target_methods)} combinations of speculative sampling...\")\n",
    "        \n",
    "        for draft_name, draft_params in tqdm(draft_methods.items()):\n",
    "            for target_name, target_params in target_methods.items():\n",
    "                combination_name = f\"{draft_name}->{target_name}\"\n",
    "                print(f\"\\nTesting combination: {combination_name}\")\n",
    "                \n",
    "                combination_results = {\n",
    "                    \"texts\": [],\n",
    "                    \"metrics\": {\n",
    "                        \"time_taken\": 0,\n",
    "                        \"tokens_per_second\": 0,\n",
    "                        \"mean_prob\": 0,\n",
    "                        \"mean_entropy\": 0,\n",
    "                        \"perplexity\": 0,\n",
    "                        \"acceptance_rate\": 0,\n",
    "                        \"speedup_factor\": 0\n",
    "                    },\n",
    "                    \"diversity\": 0,\n",
    "                    \"unique_token_ratio\": 0\n",
    "                }\n",
    "                \n",
    "                # Run multiple trials for this combination\n",
    "                total_accepted_tokens = 0\n",
    "                total_draft_tokens = 0\n",
    "                baseline_times = []\n",
    "                speculative_times = []\n",
    "                \n",
    "                for trial in range(num_trials):\n",
    "                    # First, run target model alone to get baseline performance\n",
    "                    start_time_baseline = time.time()\n",
    "                    baseline_text, baseline_metrics = self.generate_sequence(\n",
    "                        prompt, method=target_name, max_length=max_length, **target_params\n",
    "                    )\n",
    "                    baseline_time = time.time() - start_time_baseline\n",
    "                    baseline_times.append(baseline_time)\n",
    "                    \n",
    "                    # Now run speculative sampling with the combination\n",
    "                    start_time_spec = time.time()\n",
    "                    \n",
    "                    # Initialize sequence with prompt\n",
    "                    input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
    "                    token_probs = []\n",
    "                    entropy_values = []\n",
    "                    tokens_generated = 0\n",
    "                    \n",
    "                    # Generate sequence using speculative sampling\n",
    "                    for _ in range(max_length // max_draft_tokens + 1):  # Account for batching\n",
    "                        # Get current sequence\n",
    "                        current_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "                        \n",
    "                        # Draft phase: generate speculative tokens\n",
    "                        draft_tokens = []\n",
    "                        draft_logits_list = []\n",
    "                        \n",
    "                        draft_input_ids = input_ids.clone()\n",
    "                        for _ in range(max_draft_tokens):\n",
    "                            with torch.no_grad():\n",
    "                                # Get draft model logits\n",
    "                                outputs = self.model(draft_input_ids)\n",
    "                                next_token_logits = outputs.logits[:, -1, :]\n",
    "                                draft_logits_list.append(next_token_logits.clone())\n",
    "                                \n",
    "                                # Choose next token based on draft method\n",
    "                                if draft_name == \"greedy\":\n",
    "                                    next_token = self.greedy_sampling(next_token_logits)\n",
    "                                elif draft_name == \"top_k\":\n",
    "                                    next_token = self.top_k_sampling(next_token_logits, **draft_params)\n",
    "                                elif draft_name == \"top_p\":\n",
    "                                    next_token = self.top_p_sampling(next_token_logits, **draft_params)\n",
    "                                elif draft_name == \"temperature\":\n",
    "                                    next_token = self.temperature_sampling(next_token_logits, **draft_params)\n",
    "                                elif draft_name == \"importance\":\n",
    "                                    next_token = self.importance_sampling(next_token_logits, **draft_params)\n",
    "                                elif draft_name == \"rejection\":\n",
    "                                    next_token = self.rejection_sampling(next_token_logits, **draft_params)\n",
    "                                elif draft_name == \"mcmc\":\n",
    "                                    next_token = self.mcmc_sampling(next_token_logits, **draft_params)\n",
    "                                elif draft_name == \"smc\":\n",
    "                                    next_token = self.sequential_monte_carlo(next_token_logits, **draft_params)\n",
    "                                elif draft_name == \"typical\":\n",
    "                                    next_token = self.typical_sampling(next_token_logits, **draft_params)\n",
    "                                elif draft_name == \"sde\":\n",
    "                                    next_token = self.sde_sampling(next_token_logits, **draft_params)\n",
    "                                elif draft_name == \"vesde\":\n",
    "                                    next_token = self.vesde_sampling(next_token_logits, **draft_params)\n",
    "                                elif draft_name == \"eagle\":\n",
    "                                    next_token = self.eagle_sampling(next_token_logits, **draft_params)\n",
    "                                else:\n",
    "                                    raise ValueError(f\"Unknown draft sampling method: {draft_name}\")\n",
    "                            \n",
    "                            draft_tokens.append(next_token)\n",
    "                            \n",
    "                            # Add token to draft sequence\n",
    "                            next_token_tensor = torch.tensor([[next_token]]).to(self.device)\n",
    "                            draft_input_ids = torch.cat((draft_input_ids, next_token_tensor), dim=1)\n",
    "                            \n",
    "                            # Stop if we generate an end-of-sequence token\n",
    "                            if next_token == self.tokenizer.eos_token_id:\n",
    "                                break\n",
    "                        \n",
    "                        total_draft_tokens += len(draft_tokens)\n",
    "                        if not draft_tokens:\n",
    "                            break\n",
    "                        \n",
    "                        # Verification phase: check draft tokens with target model\n",
    "                        accepted_draft_tokens = []\n",
    "                        \n",
    "                        with torch.no_grad():\n",
    "                            # Get target model logits for the current sequence\n",
    "                            outputs = self.model(input_ids)\n",
    "                            target_logits = outputs.logits[:, -1, :]\n",
    "                            \n",
    "                            # Iterate through draft tokens for verification\n",
    "                            for i, draft_token in enumerate(draft_tokens):\n",
    "                                # Choose next token based on target method\n",
    "                                if target_name == \"greedy\":\n",
    "                                    target_token = self.greedy_sampling(target_logits)\n",
    "                                elif target_name == \"top_k\":\n",
    "                                    target_token = self.top_k_sampling(target_logits, **target_params)\n",
    "                                elif target_name == \"top_p\":\n",
    "                                    target_token = self.top_p_sampling(target_logits, **target_params)\n",
    "                                elif target_name == \"temperature\":\n",
    "                                    target_token = self.temperature_sampling(target_logits, **target_params)\n",
    "                                elif target_name == \"importance\":\n",
    "                                    target_token = self.importance_sampling(target_logits, **target_params)\n",
    "                                elif target_name == \"rejection\":\n",
    "                                    target_token = self.rejection_sampling(target_logits, **target_params)\n",
    "                                elif target_name == \"mcmc\":\n",
    "                                    target_token = self.mcmc_sampling(target_logits, **target_params)\n",
    "                                elif target_name == \"smc\":\n",
    "                                    target_token = self.sequential_monte_carlo(target_logits, **target_params)\n",
    "                                elif target_name == \"typical\":\n",
    "                                    target_token = self.typical_sampling(target_logits, **target_params)\n",
    "                                elif target_name == \"sde\":\n",
    "                                    target_token = self.sde_sampling(target_logits, **target_params)\n",
    "                                elif target_name == \"vesde\":\n",
    "                                    target_token = self.vesde_sampling(target_logits, **target_params)\n",
    "                                elif target_name == \"eagle\":\n",
    "                                    target_token = self.eagle_sampling(target_logits, **target_params)\n",
    "                                else:\n",
    "                                    raise ValueError(f\"Unknown target sampling method: {target_name}\")\n",
    "                                \n",
    "                                # Calculate probabilities for acceptance check\n",
    "                                target_probs = F.softmax(target_logits, dim=-1)\n",
    "                                draft_probs = F.softmax(draft_logits_list[i], dim=-1)\n",
    "                                \n",
    "                                target_prob = target_probs[0, draft_token].item()\n",
    "                                draft_prob = draft_probs[0, draft_token].item()\n",
    "                                \n",
    "                                # Acceptance test (simplified version of the test used in speculative sampling)\n",
    "                                acceptance_prob = min(1.0, target_prob / (draft_prob + 1e-10))\n",
    "                                \n",
    "                                # Decide whether to accept\n",
    "                                if np.random.random() < acceptance_prob and draft_token == target_token:\n",
    "                                    # Accept this draft token\n",
    "                                    accepted_draft_tokens.append(draft_token)\n",
    "                                    \n",
    "                                    # Calculate metrics for this token\n",
    "                                    probs = F.softmax(target_logits, dim=-1)\n",
    "                                    top_prob, _ = torch.max(probs, dim=-1)\n",
    "                                    token_probs.append(top_prob.item())\n",
    "                                    \n",
    "                                    # Calculate entropy\n",
    "                                    log_probs = torch.log(probs + 1e-10)\n",
    "                                    entropy = -torch.sum(probs * log_probs, dim=-1)\n",
    "                                    entropy_values.append(entropy.item())\n",
    "                                    \n",
    "                                    # If there are more draft tokens, get the next target logits\n",
    "                                    if i < len(draft_tokens) - 1:\n",
    "                                        next_token_tensor = torch.tensor([[draft_token]]).to(self.device)\n",
    "                                        temp_input_ids = torch.cat((input_ids, next_token_tensor), dim=1)\n",
    "                                        outputs = self.model(temp_input_ids)\n",
    "                                        target_logits = outputs.logits[:, -1, :]\n",
    "                                else:\n",
    "                                    # Reject this and all remaining draft tokens\n",
    "                                    break\n",
    "                        \n",
    "                        # Update accepted token count\n",
    "                        total_accepted_tokens += len(accepted_draft_tokens)\n",
    "                        tokens_generated += len(accepted_draft_tokens)\n",
    "                        \n",
    "                        # Add all accepted tokens to the input\n",
    "                        for token in accepted_draft_tokens:\n",
    "                            next_token_tensor = torch.tensor([[token]]).to(self.device)\n",
    "                            input_ids = torch.cat((input_ids, next_token_tensor), dim=1)\n",
    "                            \n",
    "                            # Stop if we generate an end-of-sequence token\n",
    "                            if token == self.tokenizer.eos_token_id:\n",
    "                                break\n",
    "                        \n",
    "                        # If no tokens were accepted or we hit EOS, generate one token with the target model\n",
    "                        if not accepted_draft_tokens:\n",
    "                            with torch.no_grad():\n",
    "                                outputs = self.model(input_ids)\n",
    "                                next_token_logits = outputs.logits[:, -1, :]\n",
    "                                \n",
    "                                # Choose next token based on target method\n",
    "                                if target_name == \"greedy\":\n",
    "                                    next_token = self.greedy_sampling(next_token_logits)\n",
    "                                elif target_name == \"top_k\":\n",
    "                                    next_token = self.top_k_sampling(next_token_logits, **target_params)\n",
    "                                elif target_name == \"top_p\":\n",
    "                                    next_token = self.top_p_sampling(next_token_logits, **target_params)\n",
    "                                elif target_name == \"temperature\":\n",
    "                                    next_token = self.temperature_sampling(next_token_logits, **target_params)\n",
    "                                elif target_name == \"importance\":\n",
    "                                    next_token = self.importance_sampling(next_token_logits, **target_params)\n",
    "                                elif target_name == \"rejection\":\n",
    "                                    next_token = self.rejection_sampling(next_token_logits, **target_params)\n",
    "                                elif target_name == \"mcmc\":\n",
    "                                    next_token = self.mcmc_sampling(next_token_logits, **target_params)\n",
    "                                elif target_name == \"smc\":\n",
    "                                    next_token = self.sequential_monte_carlo(next_token_logits, **target_params)\n",
    "                                elif target_name == \"typical\":\n",
    "                                    next_token = self.typical_sampling(next_token_logits, **target_params)\n",
    "                                elif target_name == \"sde\":\n",
    "                                    next_token = self.sde_sampling(next_token_logits, **target_params)\n",
    "                                elif target_name == \"vesde\":\n",
    "                                    next_token = self.vesde_sampling(next_token_logits, **target_params)\n",
    "                                elif target_name == \"eagle\":\n",
    "                                    next_token = self.eagle_sampling(next_token_logits, **target_params)\n",
    "                                \n",
    "                                # Calculate metrics for this token\n",
    "                                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                                top_prob, _ = torch.max(probs, dim=-1)\n",
    "                                token_probs.append(top_prob.item())\n",
    "                                \n",
    "                                # Calculate entropy\n",
    "                                log_probs = torch.log(probs + 1e-10)\n",
    "                                entropy = -torch.sum(probs * log_probs, dim=-1)\n",
    "                                entropy_values.append(entropy.item())\n",
    "                                \n",
    "                                next_token_tensor = torch.tensor([[next_token]]).to(self.device)\n",
    "                                input_ids = torch.cat((input_ids, next_token_tensor), dim=1)\n",
    "                                tokens_generated += 1\n",
    "                                \n",
    "                                # Stop if we generate an end-of-sequence token\n",
    "                                if next_token == self.tokenizer.eos_token_id:\n",
    "                                    break\n",
    "                    \n",
    "                    speculative_time = time.time() - start_time_spec\n",
    "                    speculative_times.append(speculative_time)\n",
    "                    \n",
    "                    # Decode the final sequence\n",
    "                    generated_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "                    combination_results[\"texts\"].append(generated_text)\n",
    "                \n",
    "                # Calculate metrics across all trials\n",
    "                acceptance_rate = total_accepted_tokens / total_draft_tokens if total_draft_tokens > 0 else 0\n",
    "                avg_baseline_time = sum(baseline_times) / len(baseline_times) if baseline_times else 0\n",
    "                avg_speculative_time = sum(speculative_times) / len(speculative_times) if speculative_times else 0\n",
    "                speedup = avg_baseline_time / avg_speculative_time if avg_speculative_time > 0 else 0\n",
    "                \n",
    "                # Calculate token-level metrics\n",
    "                if token_probs:\n",
    "                    mean_prob = np.mean(token_probs)\n",
    "                    mean_entropy = np.mean(entropy_values)\n",
    "                    perplexity = np.exp(-np.mean(np.log(token_probs)))\n",
    "                else:\n",
    "                    mean_prob = 0\n",
    "                    mean_entropy = 0\n",
    "                    perplexity = 0\n",
    "                \n",
    "                # Calculate diversity metrics\n",
    "                all_tokens = []\n",
    "                unique_tokens = set()\n",
    "                \n",
    "                for text in combination_results[\"texts\"]:\n",
    "                    tokens = self.tokenizer.encode(text)\n",
    "                    all_tokens.extend(tokens)\n",
    "                    unique_tokens.update(tokens)\n",
    "                \n",
    "                unique_token_ratio = len(unique_tokens) / len(all_tokens) if all_tokens else 0\n",
    "                \n",
    "                # Calculate pairwise diversity\n",
    "                if num_trials > 1:\n",
    "                    diversity_scores = []\n",
    "                    for i in range(num_trials):\n",
    "                        for j in range(i+1, num_trials):\n",
    "                            text_i_tokens = set(self.tokenizer.encode(combination_results[\"texts\"][i]))\n",
    "                            text_j_tokens = set(self.tokenizer.encode(combination_results[\"texts\"][j]))\n",
    "                            \n",
    "                            if not text_i_tokens or not text_j_tokens:\n",
    "                                continue\n",
    "                                \n",
    "                            # Calculate Jaccard similarity\n",
    "                            intersection = len(text_i_tokens.intersection(text_j_tokens))\n",
    "                            union = len(text_i_tokens.union(text_j_tokens))\n",
    "                            similarity = intersection / union if union > 0 else 0\n",
    "                            diversity = 1 - similarity\n",
    "                            diversity_scores.append(diversity)\n",
    "                    \n",
    "                    average_diversity = np.mean(diversity_scores) if diversity_scores else 0\n",
    "                else:\n",
    "                    average_diversity = 0\n",
    "                \n",
    "                # Update metrics\n",
    "                combination_results[\"metrics\"] = {\n",
    "                    \"time_taken\": avg_speculative_time,\n",
    "                    \"tokens_per_second\": tokens_generated / avg_speculative_time if avg_speculative_time > 0 else 0,\n",
    "                    \"mean_prob\": mean_prob,\n",
    "                    \"mean_entropy\": mean_entropy,\n",
    "                    \"perplexity\": perplexity,\n",
    "                    \"acceptance_rate\": acceptance_rate,\n",
    "                    \"speedup_factor\": speedup\n",
    "                }\n",
    "                \n",
    "                combination_results[\"diversity\"] = average_diversity\n",
    "                combination_results[\"unique_token_ratio\"] = unique_token_ratio\n",
    "                \n",
    "                results[combination_name] = combination_results\n",
    "                \n",
    "                print(f\"Results for {combination_name}:\")\n",
    "                print(f\"  Acceptance Rate: {acceptance_rate:.2f}\")\n",
    "                print(f\"  Speedup Factor: {speedup:.2f}x\")\n",
    "                print(f\"  Sample output: {combination_results['texts'][0][:100]}...\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def visualize_speculative_results(self, results):\n",
    "        \"\"\"\n",
    "        Visualize the results of speculative sampling combinations\n",
    "        \n",
    "        Args:\n",
    "            results: Dictionary with evaluation results\n",
    "        \"\"\"\n",
    "        # Extract metrics for plotting\n",
    "        combinations = list(results.keys())\n",
    "        \n",
    "        metrics_to_plot = [\n",
    "            (\"time_taken\", \"Generation Time (s)\"),\n",
    "            (\"tokens_per_second\", \"Tokens per Second\"),\n",
    "            (\"acceptance_rate\", \"Acceptance Rate\"),\n",
    "            (\"speedup_factor\", \"Speedup Factor\"),\n",
    "            (\"diversity\", \"Output Diversity\"),\n",
    "            (\"perplexity\", \"Perplexity\")\n",
    "        ]\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(3, 2, figsize=(16, 16))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, (metric_key, metric_name) in enumerate(metrics_to_plot):\n",
    "            metric_values = []\n",
    "            \n",
    "            for combo in combinations:\n",
    "                if metric_key in results[combo][\"metrics\"]:\n",
    "                    metric_values.append(results[combo][\"metrics\"][metric_key])\n",
    "                elif metric_key in results[combo]:\n",
    "                    metric_values.append(results[combo][metric_key])\n",
    "                else:\n",
    "                    metric_values.append(0)\n",
    "            \n",
    "            # Sort combinations by this metric for better visualization\n",
    "            sorted_indices = np.argsort(metric_values)\n",
    "            if metric_name in [\"Tokens per Second\", \"Acceptance Rate\", \"Speedup Factor\", \"Output Diversity\"]:\n",
    "                # For these metrics, higher is better, so reverse sort\n",
    "                sorted_indices = sorted_indices[::-1]\n",
    "            \n",
    "            sorted_combos = [combinations[i] for i in sorted_indices]\n",
    "            sorted_values = [metric_values[i] for i in sorted_indices]\n",
    "            \n",
    "            # Plot only top 10 to avoid overcrowding\n",
    "            if len(sorted_combos) > 10:\n",
    "                sorted_combos = sorted_combos[:10]\n",
    "                sorted_values = sorted_values[:10]\n",
    "            \n",
    "            axes[i].bar(sorted_combos, sorted_values)\n",
    "            axes[i].set_title(metric_name)\n",
    "            axes[i].set_xlabel(\"Sampling Combination (Draft->Target)\")\n",
    "            axes[i].set_ylabel(\"Value\")\n",
    "            axes[i].set_xticklabels(sorted_combos, rotation=45, ha=\"right\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"speculative_sampling_comparison.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Create a summary table\n",
    "        print(\"\\n===== SPECULATIVE SAMPLING COMBINATIONS SUMMARY =====\")\n",
    "        print(f\"{'Combination':<20} | {'Speed↑':<10} | {'Accept Rate':<10} | {'Diversity':<10} | {'Perplexity':<10}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Sort by speedup for the table\n",
    "        speedups = [results[combo][\"metrics\"].get(\"speedup_factor\", 0) for combo in combinations]\n",
    "        sorted_indices = np.argsort(speedups)[::-1]  # Descending order\n",
    "        \n",
    "        for idx in sorted_indices:\n",
    "            combo = combinations[idx]\n",
    "            speedup = results[combo][\"metrics\"].get(\"speedup_factor\", 0)\n",
    "            accept_rate = results[combo][\"metrics\"].get(\"acceptance_rate\", 0)\n",
    "            diversity = results[combo].get(\"diversity\", 0)\n",
    "            perplexity = results[combo][\"metrics\"].get(\"perplexity\", 0)\n",
    "            \n",
    "            print(f\"{combo:<20} | {speedup:<10.2f} | {accept_rate:<10.2f} | {diversity:<10.2f} | {perplexity:<10.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "    def top_k_sampling(self, logits, k=50):\n",
    "        \"\"\"\n",
    "        Sample from the k most likely tokens\n",
    "        \n",
    "        Args:\n",
    "            logits: Unnormalized log probabilities\n",
    "            k: Number of top tokens to consider\n",
    "            \n",
    "        Returns:\n",
    "            token_id: Id of the selected token\n",
    "        \"\"\"\n",
    "        top_k_logits, top_k_indices = torch.topk(logits, k, dim=-1)\n",
    "        probs = F.softmax(top_k_logits, dim=-1)\n",
    "        token_idx = torch.multinomial(probs, 1).item()\n",
    "        return top_k_indices[0, token_idx].item()\n",
    "    \n",
    "    def top_p_sampling(self, logits, p=0.9):\n",
    "        \"\"\"\n",
    "        Nucleus sampling - sample from the smallest set of tokens that exceed probability p\n",
    "        \n",
    "        Args:\n",
    "            logits: Unnormalized log probabilities\n",
    "            p: Cumulative probability threshold\n",
    "            \n",
    "        Returns:\n",
    "            token_id: Id of the selected token\n",
    "        \"\"\"\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        \n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        \n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        filtered_logits = logits.clone()\n",
    "        filtered_logits[0, indices_to_remove] = -float('Inf')\n",
    "        \n",
    "        probs = F.softmax(filtered_logits, dim=-1)\n",
    "        token_id = torch.multinomial(probs, 1).item()\n",
    "        return token_id\n",
    "    \n",
    "    def temperature_sampling(self, logits, temperature=0.7):\n",
    "        \"\"\"\n",
    "        Sample with temperature - lower temperature makes distribution more peaked\n",
    "        \n",
    "        Args:\n",
    "            logits: Unnormalized log probabilities\n",
    "            temperature: Temperature parameter (0-1)\n",
    "            \n",
    "        Returns:\n",
    "            token_id: Id of the selected token\n",
    "        \"\"\"\n",
    "        if temperature == 0:\n",
    "            return self.greedy_sampling(logits)\n",
    "        \n",
    "        scaled_logits = logits / temperature\n",
    "        probs = F.softmax(scaled_logits, dim=-1)\n",
    "        token_id = torch.multinomial(probs, 1).item()\n",
    "        return token_id\n",
    "    \n",
    "    def importance_sampling(self, logits, num_samples=100):\n",
    "        \"\"\"\n",
    "        Importance sampling - samples from a simpler proposal distribution\n",
    "        and weights by the ratio of target to proposal probabilities\n",
    "        \n",
    "        Args:\n",
    "            logits: Unnormalized log probabilities\n",
    "            num_samples: Number of samples to draw\n",
    "            \n",
    "        Returns:\n",
    "            token_id: Id of the selected token\n",
    "        \"\"\"\n",
    "        # Use temperature sampling as the proposal distribution\n",
    "        proposal_temp = 1.5  # Higher temperature for broader sampling\n",
    "        proposal_logits = logits / proposal_temp\n",
    "        proposal_probs = F.softmax(proposal_logits, dim=-1)\n",
    "        \n",
    "        # Target distribution\n",
    "        target_probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Sample from proposal\n",
    "        samples = torch.multinomial(proposal_probs, num_samples, replacement=True)\n",
    "        \n",
    "        # Calculate importance weights\n",
    "        weights = torch.zeros(num_samples)\n",
    "        for i in range(num_samples):\n",
    "            sample_idx = samples[0, i].item()\n",
    "            weights[i] = target_probs[0, sample_idx] / proposal_probs[0, sample_idx]\n",
    "        \n",
    "        # Normalize weights\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        # Resample according to weights\n",
    "        resampled_idx = torch.multinomial(weights, 1).item()\n",
    "        token_id = samples[0, resampled_idx].item()\n",
    "        \n",
    "        return token_id\n",
    "    \n",
    "    def rejection_sampling(self, logits, max_attempts=100):\n",
    "        \"\"\"\n",
    "        Rejection sampling - accept/reject samples based on the ratio to an upper bound\n",
    "        \n",
    "        Args:\n",
    "            logits: Unnormalized log probabilities\n",
    "            max_attempts: Maximum number of attempts before falling back\n",
    "            \n",
    "        Returns:\n",
    "            token_id: Id of the selected token\n",
    "        \"\"\"\n",
    "        # Target distribution\n",
    "        target_probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Find the maximum probability as our upper bound\n",
    "        M = torch.max(target_probs).item() * 1.1  # Add 10% margin\n",
    "        \n",
    "        # Uniform proposal distribution over the vocabulary\n",
    "        vocab_size = logits.shape[-1]\n",
    "        \n",
    "        for _ in range(max_attempts):\n",
    "            # Sample uniformly from vocabulary\n",
    "            proposal_idx = np.random.randint(0, vocab_size)\n",
    "            \n",
    "            # Acceptance probability\n",
    "            accept_prob = target_probs[0, proposal_idx].item() / M\n",
    "            \n",
    "            # Accept or reject\n",
    "            if np.random.random() < accept_prob:\n",
    "                return proposal_idx\n",
    "        \n",
    "        # Fallback to greedy sampling if max attempts reached\n",
    "        return self.greedy_sampling(logits)\n",
    "    \n",
    "    def mcmc_sampling(self, logits, num_steps=100, init_token=None):\n",
    "        \"\"\"\n",
    "        Markov Chain Monte Carlo sampling using Metropolis-Hastings algorithm\n",
    "        \n",
    "        Args:\n",
    "            logits: Unnormalized log probabilities\n",
    "            num_steps: Number of MCMC steps\n",
    "            init_token: Initial token to start the chain (random if None)\n",
    "            \n",
    "        Returns:\n",
    "            token_id: Id of the selected token\n",
    "        \"\"\"\n",
    "        # Target distribution\n",
    "        target_probs = F.softmax(logits, dim=-1).cpu().numpy().flatten()\n",
    "        vocab_size = len(target_probs)\n",
    "        \n",
    "        # Initialize with a random token if not provided\n",
    "        current_token = init_token if init_token is not None else np.random.randint(0, vocab_size)\n",
    "        \n",
    "        # Simple proposal: select a nearby token with higher probability assigned to closer tokens\n",
    "        def proposal(token):\n",
    "            # Gaussian proposal centered at current token\n",
    "            proposal = int(np.random.normal(token, vocab_size/10))\n",
    "            # Ensure it's within bounds\n",
    "            return max(0, min(vocab_size - 1, proposal))\n",
    "        \n",
    "        # Run MCMC\n",
    "        for _ in range(num_steps):\n",
    "            # Propose a new token\n",
    "            proposed_token = proposal(current_token)\n",
    "            \n",
    "            # Calculate acceptance probability\n",
    "            accept_ratio = target_probs[proposed_token] / target_probs[current_token]\n",
    "            \n",
    "            # Accept or reject\n",
    "            if np.random.random() < accept_ratio:\n",
    "                current_token = proposed_token\n",
    "                \n",
    "        return current_token\n",
    "    \n",
    "    def sequential_monte_carlo(self, logits, num_particles=100, num_steps=5):\n",
    "        \"\"\"\n",
    "        Sequential Monte Carlo (Particle Filtering) sampling\n",
    "        \n",
    "        Args:\n",
    "            logits: Unnormalized log probabilities\n",
    "            num_particles: Number of particles\n",
    "            num_steps: Number of resampling steps\n",
    "            \n",
    "        Returns:\n",
    "            token_id: Id of the selected token\n",
    "        \"\"\"\n",
    "        probs = F.softmax(logits, dim=-1).cpu().numpy().flatten()\n",
    "        vocab_size = len(probs)\n",
    "        \n",
    "        # Initialize particles randomly\n",
    "        particles = np.random.randint(0, vocab_size, size=num_particles)\n",
    "        weights = np.ones(num_particles) / num_particles\n",
    "        \n",
    "        for _ in range(num_steps):\n",
    "            # Update weights based on target distribution\n",
    "            for i in range(num_particles):\n",
    "                weights[i] = probs[particles[i]]\n",
    "            \n",
    "            # Normalize weights\n",
    "            weights = weights / np.sum(weights)\n",
    "            \n",
    "            # Resample particles based on weights\n",
    "            indices = np.random.choice(num_particles, size=num_particles, p=weights)\n",
    "            particles = particles[indices]\n",
    "            \n",
    "            # Add some noise to particles (mutation step)\n",
    "            noise = np.random.normal(0, 1, size=num_particles).astype(int)\n",
    "            particles = np.clip(particles + noise, 0, vocab_size - 1)\n",
    "            \n",
    "            # Reset weights\n",
    "            weights = np.ones(num_particles) / num_particles\n",
    "        \n",
    "        # Final weighted average (can also just take the most common particle)\n",
    "        for i in range(num_particles):\n",
    "            weights[i] = probs[particles[i]]\n",
    "        weights = weights / np.sum(weights)\n",
    "        \n",
    "        # Return the particle with the highest weight\n",
    "        return particles[np.argmax(weights)]\n",
    "    \n",
    "    def typical_sampling(self, logits, mass=0.9):\n",
    "        \"\"\"\n",
    "        Typical sampling - select tokens based on their typicality\n",
    "        (how close their information content is to the expected information)\n",
    "        \n",
    "        Args:\n",
    "            logits: Unnormalized log probabilities\n",
    "            mass: Probability mass to include\n",
    "            \n",
    "        Returns:\n",
    "            token_id: Id of the selected token\n",
    "        \"\"\"\n",
    "        # Calculate token probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Calculate entropy\n",
    "        log_probs = torch.log(probs + 1e-10)\n",
    "        expected_entropy = -torch.sum(probs * log_probs, dim=-1)\n",
    "        \n",
    "        # Calculate each token's contribution to entropy\n",
    "        token_entropies = -log_probs\n",
    "        \n",
    "        # Calculate how far each token is from the expected entropy\n",
    "        token_divergence = torch.abs(token_entropies - expected_entropy.unsqueeze(-1))\n",
    "        \n",
    "        # Sort by divergence\n",
    "        sorted_divergence, sorted_indices = torch.sort(token_divergence, dim=-1)\n",
    "        sorted_probs = probs.gather(-1, sorted_indices)\n",
    "        \n",
    "        # Keep tokens until we reach the desired probability mass\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        indices_to_keep = cumulative_probs <= mass\n",
    "        \n",
    "        # If nothing is kept, keep at least one token\n",
    "        if not torch.any(indices_to_keep):\n",
    "            indices_to_keep[0, 0] = True\n",
    "        \n",
    "        # Create a mask for the tokens to keep\n",
    "        masked_divergence = torch.full_like(token_divergence, float('inf'))\n",
    "        \n",
    "        # Fix: Ensure proper dimensionality with unsqueeze\n",
    "        indices_to_keep_original = sorted_indices.masked_select(indices_to_keep).unsqueeze(0)\n",
    "        masked_divergence.scatter_(-1, indices_to_keep_original, 0)\n",
    "        \n",
    "        # Apply the mask to the logits\n",
    "        filtered_logits = logits.clone()\n",
    "        filtered_logits[masked_divergence == float('inf')] = -float('inf')\n",
    "        \n",
    "        # Sample from the filtered logits\n",
    "        probs = F.softmax(filtered_logits, dim=-1)\n",
    "        token_id = torch.multinomial(probs, 1).item()\n",
    "        \n",
    "        return token_id\n",
    "    \n",
    "    def generate_sequence(self, prompt, method=\"greedy\", max_length=50, **kwargs):\n",
    "        \"\"\"\n",
    "        Generate a sequence using the specified sampling method\n",
    "        \n",
    "        Args:\n",
    "            prompt: Starting text prompt\n",
    "            method: Sampling method to use\n",
    "            max_length: Maximum sequence length to generate\n",
    "            **kwargs: Additional arguments for the sampling method\n",
    "            \n",
    "        Returns:\n",
    "            generated_text: The generated text sequence\n",
    "            metrics: Dictionary of performance metrics\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Tokenize the prompt\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        # Track token probabilities and entropies for evaluation\n",
    "        token_probs = []\n",
    "        entropy_values = []\n",
    "        tokens_generated = 0\n",
    "        \n",
    "        # Generate sequence token by token\n",
    "        for _ in range(max_length):\n",
    "            with torch.no_grad():\n",
    "                # Get logits for next token\n",
    "                outputs = self.model(input_ids)\n",
    "                next_token_logits = outputs.logits[:, -1, :]\n",
    "                \n",
    "                # Calculate probabilities for metrics\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                top_prob, _ = torch.max(probs, dim=-1)\n",
    "                token_probs.append(top_prob.item())\n",
    "                \n",
    "                # Calculate entropy\n",
    "                log_probs = torch.log(probs + 1e-10)\n",
    "                entropy = -torch.sum(probs * log_probs, dim=-1)\n",
    "                entropy_values.append(entropy.item())\n",
    "                \n",
    "                # Choose next token based on selected method\n",
    "                if method == \"greedy\":\n",
    "                    next_token = self.greedy_sampling(next_token_logits)\n",
    "                elif method == \"top_k\":\n",
    "                    k = kwargs.get(\"k\", 50)\n",
    "                    next_token = self.top_k_sampling(next_token_logits, k=k)\n",
    "                elif method == \"top_p\":\n",
    "                    p = kwargs.get(\"p\", 0.9)\n",
    "                    next_token = self.top_p_sampling(next_token_logits, p=p)\n",
    "                elif method.__contains__(\"temperature\"):\n",
    "                    temp = kwargs.get(\"temperature\", 0.7)\n",
    "                    next_token = self.temperature_sampling(next_token_logits, temperature=temp)\n",
    "                elif method == \"importance\":\n",
    "                    num_samples = kwargs.get(\"num_samples\", 100)\n",
    "                    next_token = self.importance_sampling(next_token_logits, num_samples=num_samples)\n",
    "                elif method == \"rejection\":\n",
    "                    max_attempts = kwargs.get(\"max_attempts\", 100)\n",
    "                    next_token = self.rejection_sampling(next_token_logits, max_attempts=max_attempts)\n",
    "                elif method == \"mcmc\":\n",
    "                    num_steps = kwargs.get(\"num_steps\", 100)\n",
    "                    next_token = self.mcmc_sampling(next_token_logits, num_steps=num_steps)\n",
    "                elif method == \"smc\":\n",
    "                    num_particles = kwargs.get(\"num_particles\", 100)\n",
    "                    num_steps = kwargs.get(\"num_steps\", 5)\n",
    "                    next_token = self.sequential_monte_carlo(next_token_logits, num_particles=num_particles, num_steps=num_steps)\n",
    "                elif method == \"typical\":\n",
    "                    mass = kwargs.get(\"mass\", 0.9)\n",
    "                    next_token = self.typical_sampling(next_token_logits, mass=mass)\n",
    "                \n",
    "                elif method == \"sde\":\n",
    "                    num_outer_steps = kwargs.get(\"num_outer_steps\", 20)\n",
    "                    num_inner_steps = kwargs.get(\"num_inner_steps\", 5)\n",
    "                    beta_min = kwargs.get(\"beta_min\", 0.1)\n",
    "                    beta_max = kwargs.get(\"beta_max\", 20.0)\n",
    "                    next_token = self.sde_sampling(next_token_logits, \n",
    "                                                num_outer_steps=num_outer_steps,\n",
    "                                                num_inner_steps=num_inner_steps,\n",
    "                                                beta_min=beta_min,\n",
    "                                                beta_max=beta_max)\n",
    "                elif method == \"vesde\":\n",
    "                    num_outer_steps = kwargs.get(\"num_outer_steps\", 20)\n",
    "                    num_inner_steps = kwargs.get(\"num_inner_steps\", 5)\n",
    "                    beta_min = kwargs.get(\"beta_min\", 0.1)\n",
    "                    beta_max = kwargs.get(\"beta_max\", 20.0)\n",
    "                    next_token = self.vesde_sampling(next_token_logits,\n",
    "                                                    num_outer_steps=num_outer_steps,\n",
    "                                                    num_inner_steps=num_inner_steps,\n",
    "                                                    beta_min=beta_min,\n",
    "                                                    beta_max=beta_max)\n",
    "                elif method == \"eagle\":\n",
    "                    draft_scale = kwargs.get(\"draft_scale\", 0.8)\n",
    "                    max_attempts = kwargs.get(\"max_attempts\", 5)\n",
    "                    next_token = self.eagle_sampling(next_token_logits, \n",
    "                                                draft_scale=draft_scale,\n",
    "                                                    max_attempts=max_attempts)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown sampling method: {method}\")\n",
    "                \n",
    "                tokens_generated += 1\n",
    "                \n",
    "                # Append the chosen token to input\n",
    "                next_token_tensor = torch.tensor([[next_token]]).to(self.device)\n",
    "                input_ids = torch.cat((input_ids, next_token_tensor), dim=1)\n",
    "                \n",
    "                # Stop if we generate an end-of-sequence token\n",
    "                if next_token == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Decode the generated sequence\n",
    "        generated_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            \"time_taken\": end_time - start_time,\n",
    "            \"tokens_per_second\": tokens_generated / (end_time - start_time) if (end_time > start_time) else 0,\n",
    "            \"mean_prob\": np.mean(token_probs) if token_probs else 0,\n",
    "            \"mean_entropy\": np.mean(entropy_values) if entropy_values else 0,\n",
    "            \"sequence_probability\": np.prod(token_probs) if token_probs else 0,\n",
    "            \"perplexity\": np.exp(-np.mean(np.log(token_probs)) if token_probs else 0)\n",
    "        }\n",
    "        \n",
    "        return generated_text, metrics\n",
    "    \n",
    "    def evaluate_methods(self, prompt, methods=None, num_generations=3, max_length=30):\n",
    "        \"\"\"\n",
    "        Evaluate multiple sampling methods on the same prompt\n",
    "        \n",
    "        Args:\n",
    "            prompt: Text prompt to start generation\n",
    "            methods: Dictionary of methods and their parameters\n",
    "            num_generations: Number of sequences to generate for each method\n",
    "            max_length: Maximum sequence length\n",
    "            \n",
    "        Returns:\n",
    "            results: Dictionary with evaluation results\n",
    "        \"\"\"\n",
    "        if methods is None:\n",
    "            methods = {\n",
    "                \"greedy\": {},\n",
    "                \"top_k\": {\"k\": 50},\n",
    "                \"top_p\": {\"p\": 0.9},\n",
    "                \"temperature\": {\"temperature\": 0.7},\n",
    "                \"importance\": {\"num_samples\": 100},\n",
    "                \"rejection\": {\"max_attempts\": 100},\n",
    "                \"mcmc\": {\"num_steps\": 50},\n",
    "                \"smc\": {\"num_particles\": 50, \"num_steps\": 3},\n",
    "                \"typical\": {\"mass\": 0.9},\n",
    "                 \"sde\": {\"num_outer_steps\": 20, \"num_inner_steps\": 5},\n",
    "                \"vesde\": {\"num_outer_steps\": 20, \"num_inner_steps\": 5},\n",
    "                \"eagle\": {\"draft_scale\": 0.8, \"max_attempts\": 5}\n",
    "            }\n",
    "        \n",
    "        results = {}\n",
    "        all_texts = defaultdict(list)\n",
    "        all_metrics = defaultdict(list)\n",
    "        \n",
    "        print(f\"Evaluating {len(methods)} sampling methods on prompt: '{prompt}'\")\n",
    "        \n",
    "        for method_name, params in tqdm(methods.items()):\n",
    "            print(f\"\\nGenerating with {method_name}...\")\n",
    "            \n",
    "            method_results = {\n",
    "                \"texts\": [],\n",
    "                \"metrics\": {},\n",
    "                \"unique_token_ratio\": 0,\n",
    "                \"diversity\": 0\n",
    "            }\n",
    "            \n",
    "            for i in range(num_generations):\n",
    "                text, metrics = self.generate_sequence(prompt, method=method_name, max_length=max_length, **params)\n",
    "                method_results[\"texts\"].append(text)\n",
    "                all_texts[method_name].append(text)\n",
    "                all_metrics[method_name].append(metrics)\n",
    "            \n",
    "            # Calculate average metrics\n",
    "            avg_metrics = {}\n",
    "            for metric in all_metrics[method_name][0].keys():\n",
    "                avg_metrics[metric] = np.mean([m[metric] for m in all_metrics[method_name]])\n",
    "            \n",
    "            # Calculate diversity metrics\n",
    "            unique_tokens = set()\n",
    "            total_tokens = 0\n",
    "            \n",
    "            for text in method_results[\"texts\"]:\n",
    "                tokens = self.tokenizer.encode(text)\n",
    "                unique_tokens.update(tokens)\n",
    "                total_tokens += len(tokens)\n",
    "            \n",
    "            method_results[\"unique_token_ratio\"] = len(unique_tokens) / total_tokens if total_tokens > 0 else 0\n",
    "            \n",
    "            # Calculate diversity using pairwise BLEU score (lower is more diverse)\n",
    "            if num_generations > 1:\n",
    "                diversity_scores = []\n",
    "                for i in range(num_generations):\n",
    "                    for j in range(i+1, num_generations):\n",
    "                        text_i_tokens = set(self.tokenizer.encode(method_results[\"texts\"][i]))\n",
    "                        text_j_tokens = set(self.tokenizer.encode(method_results[\"texts\"][j]))\n",
    "                        \n",
    "                        if not text_i_tokens or not text_j_tokens:\n",
    "                            continue\n",
    "                            \n",
    "                        # Calculate Jaccard similarity (intersection over union)\n",
    "                        intersection = len(text_i_tokens.intersection(text_j_tokens))\n",
    "                        union = len(text_i_tokens.union(text_j_tokens))\n",
    "                        similarity = intersection / union if union > 0 else 0\n",
    "                        diversity = 1 - similarity  # Convert to diversity\n",
    "                        diversity_scores.append(diversity)\n",
    "                \n",
    "                method_results[\"diversity\"] = np.mean(diversity_scores) if diversity_scores else 0\n",
    "            \n",
    "            method_results[\"metrics\"] = avg_metrics\n",
    "            results[method_name] = method_results\n",
    "            \n",
    "            print(f\"Sample output ({method_name}): {method_results['texts'][0][:100]}...\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def visualize_results(self, results, prompt_num = 1):\n",
    "        \"\"\"\n",
    "        Visualize the evaluation results\n",
    "        \n",
    "        Args:\n",
    "            results: Dictionary with evaluation results\n",
    "        \"\"\"\n",
    "        # Extract metrics for plotting\n",
    "        methods = list(results.keys())\n",
    "        \n",
    "        metrics_to_plot = [\n",
    "            (\"time_taken\", \"Generation Time (s)\"),\n",
    "            (\"tokens_per_second\", \"Tokens per Second\"),\n",
    "            (\"mean_entropy\", \"Mean Entropy\"),\n",
    "            (\"diversity\", \"Output Diversity\"),\n",
    "            (\"unique_token_ratio\", \"Unique Token Ratio\"),\n",
    "            (\"perplexity\", \"Perplexity\")\n",
    "        ]\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, (metric_key, metric_name) in enumerate(metrics_to_plot):\n",
    "            metric_values = []\n",
    "            \n",
    "            for method in methods:\n",
    "                if metric_key in results[method][\"metrics\"]:\n",
    "                    metric_values.append(results[method][\"metrics\"][metric_key])\n",
    "                elif metric_key in results[method]:\n",
    "                    metric_values.append(results[method][metric_key])\n",
    "                else:\n",
    "                    metric_values.append(0)\n",
    "            \n",
    "            axes[i].bar(methods, metric_values)\n",
    "            axes[i].set_title(metric_name)\n",
    "            axes[i].set_xlabel(\"Sampling Method\")\n",
    "            axes[i].set_ylabel(\"Value\")\n",
    "            axes[i].set_xticklabels(methods, rotation=45, ha=\"right\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"sampling_comparison.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Create a summary table\n",
    "        print(\"\\n===== SAMPLING METHODS EVALUATION SUMMARY =====\")\n",
    "        header = f\"{'Method':<12} | {'Time (s)':<10} | {'Tokens/s':<10} | {'Diversity':<10} | {'Perplexity':<10}\"\n",
    "        print(header)\n",
    "        print(\"-\" * len(header))\n",
    "        \n",
    "        for method in methods:\n",
    "            time_taken = results[method][\"metrics\"].get(\"time_taken\", 0)\n",
    "            tokens_per_sec = results[method][\"metrics\"].get(\"tokens_per_second\", 0)\n",
    "            diversity = results[method].get(\"diversity\", 0)\n",
    "            perplexity = results[method][\"metrics\"].get(\"perplexity\", 0)\n",
    "            \n",
    "            print(f\"{method:<12} | {time_taken:<10.2f} | {tokens_per_sec:<10.2f} | {diversity:<10.2f} | {perplexity:<10.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "    # S3GM inspired sampling \n",
    "    def sde_sampling(self, logits, num_outer_steps=20, num_inner_steps=5, beta_min=0.1, beta_max=20.0, **kwargs):\n",
    "        \"\"\"\n",
    "        Implements SDE-based sampling for token selection\n",
    "        \n",
    "        Args:\n",
    "            logits: The logits from which to sample the next token\n",
    "            num_outer_steps: Number of denoising steps\n",
    "            num_inner_steps: Number of corrector steps per denoising step\n",
    "            beta_min: Minimum noise level\n",
    "            beta_max: Maximum noise level\n",
    "            \n",
    "        Returns:\n",
    "            next_token: The selected token ID\n",
    "        \"\"\"\n",
    "        # Get vocabulary size from logits\n",
    "        vocab_size = logits.shape[-1]\n",
    "        \n",
    "        # Initialize with noise (similar to diffusion model approach)\n",
    "        x = torch.randn(1, vocab_size).to(self.device)\n",
    "        \n",
    "        # Helper functions for SDE sampling\n",
    "        def get_time_dependent_params(t):\n",
    "            \"\"\"Get time-dependent SDE parameters\"\"\"\n",
    "            # Linear interpolation between beta_min and beta_max\n",
    "            beta_t = beta_min + t * (beta_max - beta_min)\n",
    "            # For VESDE, the diffusion coefficient is proportional to sqrt of beta\n",
    "            sigma_t = np.sqrt(beta_t)\n",
    "            return beta_t, sigma_t\n",
    "        \n",
    "        def get_score_model(combined_logits):\n",
    "            \"\"\"Convert logits to a score function (gradient of log probability)\"\"\"\n",
    "            probs = F.softmax(combined_logits, dim=-1)\n",
    "            scores = torch.log(probs + 1e-10)\n",
    "            return scores\n",
    "        \n",
    "        def langevin_corrector(x, t, score_fn, snr=0.1):\n",
    "            \"\"\"Langevin dynamics corrector step for refining token probabilities\"\"\"\n",
    "            _, sigma_t = get_time_dependent_params(t)\n",
    "            \n",
    "            # Step size based on SNR\n",
    "            step_size = (snr * sigma_t) ** 2 * 2\n",
    "            \n",
    "            # Get score\n",
    "            score = score_fn(x)\n",
    "            \n",
    "            # Langevin dynamics update\n",
    "            noise = torch.randn_like(x)\n",
    "            x_new = x + step_size * score + np.sqrt(2 * step_size) * noise\n",
    "            \n",
    "            return x_new\n",
    "        \n",
    "        # Progressive denoising (similar to reverse diffusion process)\n",
    "        for i in range(num_outer_steps):\n",
    "            t = 1.0 - i / (num_outer_steps - 1)  # Time from 1 to 0\n",
    "            \n",
    "            # Get score function based on model logits\n",
    "            def score_fn(x_in):\n",
    "                # Combine noisy distribution with model logits\n",
    "                beta_t, _ = get_time_dependent_params(t)\n",
    "                alpha_t = 1.0 - beta_t\n",
    "                combined_logits = alpha_t * logits + np.sqrt(beta_t) * x_in\n",
    "                return get_score_model(combined_logits)\n",
    "            \n",
    "            # Apply corrector steps (Langevin dynamics)\n",
    "            for _ in range(num_inner_steps):\n",
    "                x = langevin_corrector(x, t, score_fn)\n",
    "        \n",
    "        # Final token selection\n",
    "        combined_logits = logits + 0.01 * x  # Small noise addition for exploration\n",
    "        probs = F.softmax(combined_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "        \n",
    "        return next_token\n",
    "\n",
    "\n",
    "     \n",
    "    def vesde_sampling(self, logits, num_outer_steps=20, num_inner_steps=5, beta_min=0.1, beta_max=20.0, **kwargs):\n",
    "        \"\"\"\n",
    "        Generate next token using Variance Exploding SDE sampling\n",
    "        \n",
    "        Args:\n",
    "            logits: The logits from which to sample the next token\n",
    "            num_outer_steps: Number of denoising steps\n",
    "            num_inner_steps: Number of corrector steps per denoising step\n",
    "            beta_min: Minimum noise level\n",
    "            beta_max: Maximum noise level\n",
    "            \n",
    "        Returns:\n",
    "            next_token: The selected token ID\n",
    "        \"\"\"\n",
    "        # Get vocabulary size from logits\n",
    "        vocab_size = logits.shape[-1]\n",
    "        \n",
    "        # Initialize with pure noise\n",
    "        x = torch.randn(1, vocab_size).to(self.device) * beta_max\n",
    "        \n",
    "        # Helper function to get time-dependent parameters\n",
    "        def get_time_dependent_params(t):\n",
    "            beta_t = beta_min + t * (beta_max - beta_min)\n",
    "            sigma_t = np.sqrt(beta_t)\n",
    "            return beta_t, sigma_t\n",
    "        \n",
    "        # Discretized reverse-time SDE\n",
    "        time_steps = torch.linspace(1.0, 0.0, num_outer_steps).to(self.device)\n",
    "        for i in range(len(time_steps) - 1):\n",
    "            t = time_steps[i]\n",
    "            dt = time_steps[i] - time_steps[i + 1]\n",
    "            \n",
    "            # Current noise level\n",
    "            beta_t, sigma_t = get_time_dependent_params(t.item())\n",
    "            \n",
    "            # Convert to tensor for pytorch operations\n",
    "            beta_t_tensor = torch.tensor(beta_t, device=self.device)\n",
    "            sigma_t_tensor = torch.tensor(sigma_t, device=self.device)\n",
    "            \n",
    "            # Get score estimate (gradient of log probability)\n",
    "            probs = F.softmax(logits + x / (sigma_t ** 2), dim=-1)\n",
    "            score = torch.log(probs + 1e-10)\n",
    "            \n",
    "            # Drift term\n",
    "            drift = -0.5 * beta_t_tensor * x\n",
    "            \n",
    "            # Diffusion term\n",
    "            diffusion = torch.sqrt(beta_t_tensor) * torch.randn_like(x)\n",
    "            \n",
    "            # Update using Euler-Maruyama discretization\n",
    "            x = x + (drift + 0.5 * (sigma_t_tensor ** 2) * score) * dt + torch.sqrt(dt) * diffusion\n",
    "            \n",
    "            # Apply corrector steps (Langevin dynamics)\n",
    "            for _ in range(num_inner_steps):\n",
    "                noise_scale = torch.sqrt(torch.tensor(2.0) * beta_t_tensor * 0.1)  # 0.1 is step size\n",
    "                noise = torch.randn_like(x) * noise_scale\n",
    "                \n",
    "                # Get score\n",
    "                probs = F.softmax(logits + x / (sigma_t ** 2), dim=-1)\n",
    "                score = torch.log(probs + 1e-10)\n",
    "                \n",
    "                # Update\n",
    "                x = x + beta_t_tensor * score * 0.1 + noise\n",
    "        \n",
    "        # Final selection based on combined logits and noise\n",
    "        probs = F.softmax(logits + x * 0.01, dim=-1)  # Small noise influence\n",
    "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "        \n",
    "        return next_token\n",
    "    \n",
    "    # EAGLE sampling from the first suggested sampling research paper\n",
    "    def eagle_sampling(self, logits, draft_scale=0.8, max_attempts=5, **kwargs):\n",
    "        \"\"\"\n",
    "        Implements EAGLE (Efficient Accelerated Generation with Lightweight Encoders) sampling\n",
    "        \n",
    "        Args:\n",
    "            logits: Logits from the target model\n",
    "            draft_scale: Scale factor for the draft model (simulated)\n",
    "            max_attempts: Maximum number of draft tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            next_token: The selected token ID\n",
    "        \"\"\"\n",
    "        # In a real implementation, we would have a separate draft model\n",
    "        # Here we'll simulate one by adding noise to the target model's logits\n",
    "        \n",
    "        # Get vocabulary size\n",
    "        vocab_size = logits.shape[-1]\n",
    "        \n",
    "        # Create a simulated \"draft model\" by adding noise to the target logits\n",
    "        # This simulates having a smaller, less accurate model\n",
    "        noise = torch.randn_like(logits) * 0.2  # Noise level\n",
    "        draft_logits = logits * draft_scale + noise\n",
    "        \n",
    "        # Get probabilities from both models\n",
    "        target_probs = F.softmax(logits, dim=-1)\n",
    "        draft_probs = F.softmax(draft_logits, dim=-1)\n",
    "        \n",
    "        # Draft phase: generate candidate tokens from draft model\n",
    "        # In real EAGLE, we'd generate multiple tokens at once\n",
    "        # For simplicity, we'll just sample tokens until one is accepted\n",
    "        for _ in range(max_attempts):\n",
    "            # Sample from draft model\n",
    "            draft_token = torch.multinomial(draft_probs, num_samples=1).item()\n",
    "            \n",
    "            # Verification phase: compute acceptance probability\n",
    "            target_prob = target_probs[0, draft_token].item()\n",
    "            draft_prob = draft_probs[0, draft_token].item()\n",
    "            \n",
    "            # Calculate acceptance probability (min to ensure it's <= 1.0)\n",
    "            # This is a simplified version of the acceptance test\n",
    "            acceptance_prob = min(1.0, target_prob / (draft_prob + 1e-10))\n",
    "            \n",
    "            # Accept or reject\n",
    "            if torch.rand(1).item() < acceptance_prob:\n",
    "                return draft_token\n",
    "        \n",
    "        # If no draft tokens were accepted, fall back to sampling from target model\n",
    "        return torch.multinomial(target_probs, num_samples=1).item()   \n",
    "\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# class SemanticEvaluator:\n",
    "#     def __init__(self, openai_api_key: str = None):\n",
    "#         \"\"\"\n",
    "#         Initialize the semantic evaluator with OpenAI API\n",
    "        \n",
    "#         Args:\n",
    "#             openai_api_key (str, optional): OpenAI API key. \n",
    "#             If not provided, will try to read from environment variable.\n",
    "#         \"\"\"\n",
    "#         if openai_api_key is None:\n",
    "#             openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "        \n",
    "#         if not openai_api_key:\n",
    "#             raise ValueError(\"OpenAI API key must be provided either as argument or in OPENAI_API_KEY environment variable\")\n",
    "        \n",
    "#         openai.api_key = openai_api_key\n",
    "\n",
    "#     def evaluate_response(\n",
    "#         self, \n",
    "#         original_prompt: str, \n",
    "#         response: str, \n",
    "#         model: str = \"gpt-3.5-turbo\"\n",
    "#     ) -> Dict[str, Any]:\n",
    "#         \"\"\"\n",
    "#         Evaluate a response's semantic quality using OpenAI's model\n",
    "        \n",
    "#         Args:\n",
    "#             original_prompt (str): The original input prompt\n",
    "#             response (str): The generated response to evaluate\n",
    "#             model (str, optional): OpenAI model to use for evaluation\n",
    "        \n",
    "#         Returns:\n",
    "#             Dict containing evaluation metrics\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             # Construct a detailed evaluation prompt\n",
    "#             evaluation_prompt = f\"\"\"\n",
    "#             Please evaluate the following response to the original prompt:\n",
    "\n",
    "#             Original Prompt: \"{original_prompt}\"\n",
    "#             Response: \"{response}\"\n",
    "\n",
    "#             Provide a detailed evaluation focusing on:\n",
    "#             1. Grammar (Score 0-10)\n",
    "#             2. Coherence/Making Sense (Score 0-10)\n",
    "#             3. Completeness of Answer (Score 0-10)\n",
    "#             4. Brief explanation for each score\n",
    "\n",
    "#             Respond in a JSON format:\n",
    "#             {{\n",
    "#                 \"grammar_score\": [0-10],\n",
    "#                 \"coherence_score\": [0-10],\n",
    "#                 \"completeness_score\": [0-10],\n",
    "#                 \"grammar_explanation\": \"...\",\n",
    "#                 \"coherence_explanation\": \"...\",\n",
    "#                 \"completeness_explanation\": \"...\"\n",
    "#             }}\n",
    "#             \"\"\"\n",
    "\n",
    "#             # Make API call to OpenAI\n",
    "#             response = openai.ChatCompletion.create(\n",
    "#                 model=model,\n",
    "#                 response_format={\"type\": \"json_object\"},\n",
    "#                 messages=[\n",
    "#                     {\"role\": \"system\", \"content\": \"You are a precise evaluator of text responses.\"},\n",
    "#                     {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "#                 ],\n",
    "#                 temperature=0.2  # Low temperature for consistent evaluation\n",
    "#             )\n",
    "\n",
    "#             # Parse the JSON response\n",
    "#             evaluation = json.loads(response.choices[0].message.content)\n",
    "            \n",
    "#             # Calculate overall semantic score\n",
    "#             evaluation['semantic_score'] = (\n",
    "#                 evaluation.get('grammar_score', 0) + \n",
    "#                 evaluation.get('coherence_score', 0) + \n",
    "#                 evaluation.get('completeness_score', 0)\n",
    "#             ) / 3\n",
    "\n",
    "#             return evaluation\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error in semantic evaluation: {e}\")\n",
    "#             return {\n",
    "#                 \"error\": str(e),\n",
    "#                 \"semantic_score\": 0\n",
    "#             }\n",
    "\n",
    "#     def batch_evaluate_responses(\n",
    "#         self, \n",
    "#         original_prompt: str, \n",
    "#         responses: List[str], \n",
    "#         model: str = \"gpt-3.5-turbo\"\n",
    "#     ) -> List[Dict[str, Any]]:\n",
    "#         \"\"\"\n",
    "#         Batch evaluate multiple responses\n",
    "        \n",
    "#         Args:\n",
    "#             original_prompt (str): The original input prompt\n",
    "#             responses (List[str]): List of responses to evaluate\n",
    "#             model (str, optional): OpenAI model to use for evaluation\n",
    "        \n",
    "#         Returns:\n",
    "#             List of evaluation dictionaries\n",
    "#         \"\"\"\n",
    "#         return [\n",
    "#             self.evaluate_response(original_prompt, response, model) \n",
    "#             for response in responses\n",
    "#         ]\n",
    "\n",
    "# def main():\n",
    "#     # Example usage\n",
    "#     openai_api_key = os.getenv('OPENAI_API_KEY')  # Make sure to set this environment variable\n",
    "#     evaluator = SemanticEvaluator(openai_api_key)\n",
    "\n",
    "#     # Sample prompts and responses\n",
    "#     prompts_and_responses = [\n",
    "#         {\n",
    "#             \"prompt\": \"Explain the concept of artificial intelligence\",\n",
    "#             \"responses\": [\n",
    "#                 \"AI is a technology that makes computers smart.\",\n",
    "#                 \"Artificial Intelligence (AI) is a sophisticated field of computer science that focuses on creating intelligent machines capable of simulating human-like cognitive processes, including learning, problem-solving, perception, and decision-making. It encompasses various subfields like machine learning, neural networks, and deep learning, enabling systems to analyze complex data, recognize patterns, and make autonomous decisions across diverse domains such as healthcare, finance, robotics, and more.\"\n",
    "#             ]\n",
    "#         },\n",
    "#         {\n",
    "#             \"prompt\": \"What is the capital of France?\",\n",
    "#             \"responses\": [\n",
    "#                 \"Paris is the capital.\",\n",
    "#                 \"Pari is the captial of Fronce\"\n",
    "#             ]\n",
    "#         }\n",
    "#     ]\n",
    "\n",
    "#     # Evaluate responses\n",
    "#     for item in prompts_and_responses:\n",
    "#         print(f\"\\nPrompt: {item['prompt']}\")\n",
    "#         evaluations = evaluator.batch_evaluate_responses(\n",
    "#             item['prompt'], \n",
    "#             item['responses']\n",
    "#         )\n",
    "        \n",
    "#         for i, (response, evaluation) in enumerate(zip(item['responses'], evaluations), 1):\n",
    "#             print(f\"\\nResponse {i}: {response}\")\n",
    "#             print(\"Evaluation:\")\n",
    "#             for key, value in evaluation.items():\n",
    "#                 print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "# class SemanticEvaluator:\n",
    "#     def __init__(self, api_key: str = None):\n",
    "#         \"\"\"\n",
    "#         Initialize the semantic evaluator with OpenAI API client\n",
    "        \n",
    "#         Args:\n",
    "#             api_key (str, optional): OpenAI API key. \n",
    "#             If not provided, will try to read from environment variable.\n",
    "#         \"\"\"\n",
    "#         # Use OpenAI() client initialization\n",
    "#         if api_key is None:\n",
    "#               api_key = os.getenv('OPENAI_API_KEY')\n",
    "#         self.client = openai.OpenAI(api_key=api_key)\n",
    "#         openai.api_key = api_key\n",
    "\n",
    "#     def evaluate_response(\n",
    "#         self, \n",
    "#         original_prompt: str, \n",
    "#         response: str, \n",
    "#         model: str = \"gpt-4o\"\n",
    "#     ) -> Dict[str, Any]:\n",
    "#         \"\"\"\n",
    "#         Evaluate a response's semantic quality using OpenAI's model\n",
    "        \n",
    "#         Args:\n",
    "#             original_prompt (str): The original input prompt\n",
    "#             response (str): The generated response to evaluate\n",
    "#             model (str, optional): OpenAI model to use for evaluation\n",
    "        \n",
    "#         Returns:\n",
    "#             Dict containing evaluation metrics\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             # Construct a detailed evaluation prompt\n",
    "#             completion = self.client.chat.completions.create(\n",
    "#                 model=model,\n",
    "#                 response_format={\"type\": \"json_object\"},\n",
    "#                 messages=[\n",
    "#                     {\n",
    "#                         \"role\": \"system\", \n",
    "#                         \"content\": \"You are a precise evaluator of text responses. Provide a JSON evaluation.\"\n",
    "#                     },\n",
    "#                     {\n",
    "#                         \"role\": \"user\", \n",
    "#                         \"content\": f\"\"\"\n",
    "#                         Please evaluate the following response to the original prompt:\n",
    "\n",
    "#                         Original Prompt: \"{original_prompt}\"\n",
    "#                         Response: \"{response}\"\n",
    "\n",
    "#                         Provide a detailed evaluation focusing on:\n",
    "#                         1. Grammar (Score 0-10)\n",
    "#                         2. Coherence/Making Sense (Score 0-10)\n",
    "#                         3. Completeness of Answer (Score 0-10)\n",
    "#                         4. Brief explanation for each score\n",
    "\n",
    "#                         Respond strictly in this JSON format:\n",
    "#                         {{\n",
    "#                             \"grammar_score\": [0-10],\n",
    "#                             \"coherence_score\": [0-10],\n",
    "#                             \"completeness_score\": [0-10],\n",
    "#                             \"grammar_explanation\": \"...\",\n",
    "#                             \"coherence_explanation\": \"...\",\n",
    "#                             \"completeness_explanation\": \"...\"\n",
    "#                         }}\n",
    "#                         \"\"\"\n",
    "#                     }\n",
    "#                 ],\n",
    "#                 temperature=0.2  # Low temperature for consistent evaluation\n",
    "#             )\n",
    "\n",
    "#             # Parse the JSON response\n",
    "#             evaluation = json.loads(completion.choices[0].message.content)\n",
    "            \n",
    "#             # Calculate overall semantic score\n",
    "#             evaluation['semantic_score'] = (\n",
    "#                 evaluation.get('grammar_score', 0) + \n",
    "#                 evaluation.get('coherence_score', 0) + \n",
    "#                 evaluation.get('completeness_score', 0)\n",
    "#             ) / 3\n",
    "\n",
    "#             return evaluation\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error in semantic evaluation: {e}\")\n",
    "#             return {\n",
    "#                 \"error\": str(e),\n",
    "#                 \"semantic_score\": 0\n",
    "#             }\n",
    "\n",
    "#     def batch_evaluate_responses(\n",
    "#         self, \n",
    "#         original_prompt: str, \n",
    "#         responses: List[str], \n",
    "#         model: str = \"gpt-4o\"\n",
    "#     ) -> List[Dict[str, Any]]:\n",
    "#         \"\"\"\n",
    "#         Batch evaluate multiple responses\n",
    "        \n",
    "#         Args:\n",
    "#             original_prompt (str): The original input prompt\n",
    "#             responses (List[str]): List of responses to evaluate\n",
    "#             model (str, optional): OpenAI model to use for evaluation\n",
    "        \n",
    "#         Returns:\n",
    "#             List of evaluation dictionaries\n",
    "#         \"\"\"\n",
    "#         return [\n",
    "#             self.evaluate_response(original_prompt, response, model) \n",
    "#             for response in responses\n",
    "        # ]\n",
    "\n",
    "def main():\n",
    "    # Initialize the evaluator\n",
    "    evaluator = SamplingEvaluator(model_name=\n",
    "                                  \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "        # model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "    prompts = []\n",
    "    # Define prompt\n",
    "    # prompts.append(\"Artificial intelligence has transformed the way we\")\n",
    "    # prompts.append(\"What is the capital of France? \")\n",
    "    prompts.append(\"what is 2+5?\")\n",
    "    # prompts.append(\"WHAT IS THE ANSWER TO TWO PLUS FIVE?\")\n",
    "    # prompts.append(\"artifitial  intelige has chang the way we\")\n",
    "    # prompts.append(\"hey , how are you?\")\n",
    "    # prompts.append(\"Artificial intelligence has transformed the way we live, work, and interact by revolutionizing countless industries and enhancing daily experiences. \" \\\n",
    "    # \"From enabling personalized recommendations in entertainment and shopping to streamlining operations in healthcare and finance, AI has reshaped efficiency and decision-making. It powers autonomous vehicles, advances scientific research, and augments human creativity with tools for art, writing, and design. Moreover, AI fosters innovation by automating complex tasks, analyzing vast datasets, and uncovering patterns that were once beyond human reach.\" \\\n",
    "    # \" This transformative technology is seamlessly integrating into our lives, shaping a smarter, more connected future.\")\n",
    "    #  # Example usage\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # response_evaluator = SemanticEvaluator(openai_api_key)\n",
    "    prompts_and_responses = []\n",
    "    for prompt in prompts:\n",
    "\n",
    "        #experimment with coming up with something such a joke \n",
    "        \n",
    "        # ask mathematical questions and see if they can answer them\n",
    "        #check which one works better bad questions (grammar , vagueness , etc)\n",
    "        #single answer vs long answer\n",
    "        #formal vs non formal \n",
    "        #longer vs shorter prompt \n",
    "        \n",
    "        # Define methods to evaluate with parameters\n",
    "        # methods = {\n",
    "        #     \"greedy\": {},\n",
    "        #     \"top_k\": {\"k\": 50},\n",
    "        #     \"top_p\": {\"p\": 0.9},\n",
    "        #     \"temperature\": {\"temperature\": 0.7},\n",
    "        #     \"importance\": {\"num_samples\": 100},\n",
    "        #     \"rejection\": {\"max_attempts\": 100},\n",
    "        #     \"mcmc\": {\"num_steps\": 50},\n",
    "        #     \"smc\": {\"num_particles\": 50, \"num_steps\": 3},\n",
    "        #     \"typical\": {\"mass\": 0.9},\n",
    "        #     \"sde\": {\"num_outer_steps\": 20, \"num_inner_steps\": 5},\n",
    "        #     \"vesde\": {\"num_outer_steps\": 20, \"num_inner_steps\": 5},\n",
    "        #     \"eagle\": {\"draft_scale\": 0.8, \"max_attempts\": 5}\n",
    "        # }\n",
    "        methods = {\n",
    "            \"temperature1\": {\"temperature\": 0.1},\n",
    "            \"temperature2\": {\"temperature\": 0.2},\n",
    "            \"temperature3\": {\"temperature\": 0.3},\n",
    "            \"temperature4\": {\"temperature\": 0.4},\n",
    "            \"temperature5\": {\"temperature\": 0.5},\n",
    "            \"temperature6\": {\"temperature\": 0.6},\n",
    "            \"temperature7\": {\"temperature\": 0.7},\n",
    "            \"temperature8\": {\"temperature\": 0.8},\n",
    "            \"temperature9\": {\"temperature\": 0.9},\n",
    "            \"temperature10\": {\"temperature\": 1},\n",
    "            \n",
    "        }\n",
    "        print(len(methods))\n",
    "        # Evaluate all methods\n",
    "        results = evaluator.evaluate_methods(prompt, methods, num_generations=3)\n",
    "        # Visualize results\n",
    "        evaluator.visualize_results(results)\n",
    "\n",
    "        \n",
    "        # Print a sample of each method's output\n",
    "        print(\"\\n===== SAMPLE OUTPUTS =====\")\n",
    "        for method, result in results.items():\n",
    "            print(f\"\\n{method.upper()}:\")\n",
    "            print(result[\"texts\"][0])\n",
    "            print(result)\n",
    "            print(result[\"texts\"])\n",
    "\n",
    "            obj_openai_eval = {\n",
    "            \"prompt\": prompt,\n",
    "            \"responses\": [\n",
    "                result[\"texts\"][0]\n",
    "            ]\n",
    "            }\n",
    "            prompts_and_responses.append(obj_openai_eval)\n",
    "\n",
    "            \n",
    "            # response_evaluator.evaluate_response()\n",
    "\n",
    "\n",
    "        # for item in prompts_and_responses:\n",
    "        #     print(f\"\\nPrompt: {item['prompt']}\")\n",
    "        #     evaluations = response_evaluator.batch_evaluate_responses(\n",
    "        #         item['prompt'], \n",
    "        #         item['responses']\n",
    "        #     )\n",
    "            \n",
    "        #     for i, (response, evaluation) in enumerate(zip(item['responses'], evaluations), 1):\n",
    "        #         print(f\"\\nResponse {i}: {response}\")\n",
    "        #         print(\"Evaluation:\")\n",
    "        #         for key, value in evaluation.items():\n",
    "        #             print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "    \n",
    "# https://www.scikit-yb.org/en/latest/api/text/freqdist.html#token-frequency-distribution\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
